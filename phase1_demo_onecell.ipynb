{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, asyncio\nfrom pathlib import Path\nimport nest_asyncio\n\n# 修改为你的仓库路径\nrepo_root = Path(\"/content/cv_project\")\nsys.path.insert(0, str(repo_root / \"src\"))\nos.environ[\"PYTHONPATH\"] = str(repo_root / \"src\")\n\n# 允许在已运行的 event loop 中再次 run_until_complete\nnest_asyncio.apply()\n\nfrom core.config import SystemConfig\nfrom pipeline.video_semantic_search import VideoSemanticSystem\n\nconfig = SystemConfig(\n    video_path=repo_root / \"data/raw/semantic/MOT17-12.mp4\",\n    output_dir=repo_root / \"output/demo_run\",\n    vlm_backend=\"vllm\",\n    vllm_endpoint=\"http://localhost:8000/v1\",\n    vllm_model_name=\"Qwen/Qwen3-VL-4B-Instruct\",\n    router_backend=\"vllm\",\n)\n\nquestion = \"Find the person wearing a red plaid shirt and black pants. Are they using a phone? Are they walking or running, and in which direction (from where to where)?\"\n\nsystem = VideoSemanticSystem(config=config)\n# 重写 _run_coroutine，避免 asyncio.run 冲突\nsystem._run_coroutine = lambda coro: asyncio.get_event_loop().run_until_complete(coro)\n\nprint(\"=== Building index ===\")\nsystem.build_index()\nprint(f\"Evidence packages: {len(system.evidence_map)}\")\n\nprint(f\"=== Running query: {question} ===\")\nresults = system.question_search(question, top_k=3)\nprint(f\"Matches: {len(results)}\")\nfor r in results:\n    print(f\"track {r.track_id}: {r.start_s:.1f}s–{r.end_s:.1f}s | score={r.score:.2f} | {r.reason}\")\n\nprint(\"Outputs in output/demo_run/: tracking_<question>.mp4 and tracking_all_tracks_<question>.mp4\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}