# VLM扩展依赖（在基础requirements.txt之上）
# 用于semantic_vlm_vllm.py

# === 方案1：vLLM加速（推荐，如果兼容） ===
vllm>=0.6.0  # vLLM推理引擎
qwen-vl-utils  # Qwen2-VL工具包

# === 方案2：transformers降级（兼容性好） ===
# 如果vLLM在M4上不work，会自动使用这个
# transformers>=4.37.0  # 已在基础requirements.txt中
# torch>=2.0.0  # 已在基础requirements.txt中

# === 额外工具 ===
einops  # 张量操作（VLM需要）
timm  # 视觉模型（某些VLM依赖）

# === 注意事项 ===
# 1. vLLM主要为CUDA优化，M4上可能：
#    - 自动降级到CPU模式（慢）
#    - 或者报错（代码会自动切换到transformers）
#
# 2. 首次运行会下载模型：
#    - Qwen2-VL-2B: 约5GB
#    - 需要稳定网络
#
# 3. 内存需求：
#    - vLLM模式: ~5-6GB
#    - transformers模式: ~5GB
#    - M4 16GB可以运行
#
# 4. 安装顺序：
#    pip install -r requirements.txt  # 先安装基础依赖
#    pip install -r requirements_vlm.txt  # 再安装VLM扩展

