# å®Œæ•´çš„è¯­ä¹‰æŸ¥è¯¢ç³»ç»Ÿ
# ç›®æ ‡ï¼šè·Ÿè¸ª â†’ æå–äººç‰© â†’ CLIPç¼–ç  â†’ è¯­ä¹‰æŸ¥è¯¢

import cv2
import json
import torch
import numpy as np
from pathlib import Path
from PIL import Image
from ultralytics import YOLO
from boxmot import create_tracker
from transformers import CLIPProcessor, CLIPModel

print("=" * 70)
print("ğŸ¯ è¯­ä¹‰æŸ¥è¯¢ç³»ç»Ÿï¼šç”¨äººè¯æ‰¾äºº")
print("=" * 70)

# ===== é…ç½® =====
VIDEO_PATH = Path("/Users/neko_wen/my/ä»£ç /uw/cv/project/data/snippets/debug_15s.mp4")
OUTPUT_DIR = Path("output_semantic")
SAMPLE_FRAMES = 5  # æ¯ä¸ªtrack_idé‡‡æ ·å‡ å¸§

OUTPUT_DIR.mkdir(exist_ok=True)

# ===== ç¬¬ä¸€æ­¥ï¼šè·Ÿè¸ª + æå–äººç‰©å›¾ç‰‡ =====
print("\n" + "=" * 70)
print("ğŸ“¹ ç¬¬ä¸€æ­¥ï¼šè·Ÿè¸ªå¹¶æå–äººç‰©å›¾ç‰‡")
print("=" * 70)

# åˆå§‹åŒ–
model = YOLO("yolov8n.pt")
tracker = create_tracker(
    tracker_type='bytetrack',
    tracker_config=None,
    reid_weights=None,
    device='cpu',
    half=False,
    per_class=False
)

cap = cv2.VideoCapture(str(VIDEO_PATH))
fps = cap.get(cv2.CAP_PROP_FPS)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

print(f"\nğŸ“¹ è§†é¢‘: {VIDEO_PATH.name}")
print(f"   æ€»å¸§æ•°: {total_frames}")

# å­˜å‚¨æ¯ä¸ªtrack_idçš„å›¾ç‰‡
track_images = {}  # {track_id: [å›¾ç‰‡1, å›¾ç‰‡2, ...]}
track_frames = {}  # {track_id: [å¸§å·1, å¸§å·2, ...]}

frame_count = 0
crops_dir = OUTPUT_DIR / "crops"
crops_dir.mkdir(exist_ok=True)

print("\nğŸš€ å¼€å§‹è·Ÿè¸ªå’Œæå–...")

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    frame_count += 1
    
    # YOLOæ£€æµ‹
    results = model.predict(
        source=frame,
        device="mps",
        conf=0.3,
        classes=[0],
        verbose=False
    )
    
    detections = []
    boxes = results[0].boxes
    if boxes is not None and len(boxes) > 0:
        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = float(box.conf[0])
            cls_id = int(box.cls[0])
            detections.append([x1, y1, x2, y2, conf, cls_id])
    
    # è·Ÿè¸ª
    if len(detections) > 0:
        detections = np.array(detections)
        tracks = tracker.update(detections, frame)
        
        if tracks.size > 0:
            for track in tracks:
                x1, y1, x2, y2 = map(int, track[:4])
                track_id = int(track[4])
                
                # åˆå§‹åŒ–track_id
                if track_id not in track_images:
                    track_images[track_id] = []
                    track_frames[track_id] = []
                
                # é‡‡æ ·ï¼šæ¯ä¸ªIDåªä¿å­˜SAMPLE_FRAMESå¼ å›¾
                if len(track_images[track_id]) < SAMPLE_FRAMES:
                    # è£å‰ªäººç‰©å›¾ç‰‡
                    # æ‰©å¤§è¾¹ç•Œæ¡†10%
                    pad = 10
                    x1 = max(0, x1 - pad)
                    y1 = max(0, y1 - pad)
                    x2 = min(width, x2 + pad)
                    y2 = min(height, y2 + pad)
                    
                    crop = frame[y1:y2, x1:x2]
                    
                    if crop.size > 0:
                        # ä¿å­˜
                        track_images[track_id].append(crop)
                        track_frames[track_id].append(frame_count)
                        
                        # ä¿å­˜åˆ°æ–‡ä»¶
                        img_path = crops_dir / f"id{track_id:03d}_frame{frame_count:04d}.jpg"
                        cv2.imwrite(str(img_path), crop)
    
    if frame_count % 50 == 0:
        print(f"â³ å¤„ç†å¸§:{frame_count}/{total_frames} | å·²æå–IDæ•°:{len(track_images)}")

cap.release()

print(f"\nâœ… æå–å®Œæˆï¼")
print(f"   å”¯ä¸€IDæ•°: {len(track_images)}")
print(f"   æ€»å›¾ç‰‡æ•°: {sum(len(imgs) for imgs in track_images.values())}")

# ===== ç¬¬äºŒæ­¥ï¼šCLIPç‰¹å¾æå– =====
print("\n" + "=" * 70)
print("ğŸ§  ç¬¬äºŒæ­¥ï¼šCLIPç‰¹å¾æå–")
print("=" * 70)

print("\nğŸ“¦ åŠ è½½CLIPæ¨¡å‹...")
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
print("âœ… CLIPåŠ è½½å®Œæˆ")

# ä¸ºæ¯ä¸ªtrack_idçš„æ¯å¼ å›¾ç‰‡ç”Ÿæˆç‰¹å¾å‘é‡
features_db = {}  # {track_id: [å‘é‡1, å‘é‡2, ...]}

print("\nğŸ”„ æå–ç‰¹å¾å‘é‡...")
for track_id, images in track_images.items():
    features_db[track_id] = []
    
    for img in images:
        # OpenCV BGR â†’ PIL RGB
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(img_rgb)
        
        # CLIPç¼–ç 
        inputs = clip_processor(images=pil_img, return_tensors="pt")
        with torch.no_grad():
            image_features = clip_model.get_image_features(**inputs)
            # å½’ä¸€åŒ–
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            features_db[track_id].append(image_features[0].numpy())
    
    if track_id % 5 == 0:
        print(f"   å¤„ç†ID:{track_id} ({len(images)}å¼ å›¾)")

print(f"\nâœ… ç‰¹å¾æå–å®Œæˆï¼")

# ä¿å­˜ç‰¹å¾æ•°æ®åº“
features_file = OUTPUT_DIR / "features.npz"
np.savez(features_file, **{f"id_{k}": np.array(v) for k, v in features_db.items()})
print(f"   ç‰¹å¾åº“å·²ä¿å­˜: {features_file}")

# ä¿å­˜å…ƒæ•°æ®
metadata = {
    "track_ids": list(track_images.keys()),
    "num_images_per_id": {k: len(v) for k, v in track_images.items()},  # ä¿®å¤ï¼šæ”¹ä¸º.items()
    "video": str(VIDEO_PATH)
}
with open(OUTPUT_DIR / "metadata.json", 'w') as f:
    json.dump(metadata, f, indent=2)

# ===== ç¬¬ä¸‰æ­¥ï¼šè¯­ä¹‰æŸ¥è¯¢ =====
print("\n" + "=" * 70)
print("ğŸ” ç¬¬ä¸‰æ­¥ï¼šè¯­ä¹‰æŸ¥è¯¢")
print("=" * 70)

def search(query_text, top_k=5, threshold=0.25):
    """
    è¯­ä¹‰æŸ¥è¯¢å‡½æ•°
    
    å‚æ•°:
        query_text: æŸ¥è¯¢æ–‡æœ¬ï¼ˆå¦‚ï¼š"ç©¿çº¢è‰²è¡£æœçš„äºº"ï¼‰
        top_k: è¿”å›å‰kä¸ªç»“æœ
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆåªè¿”å›é«˜äºæ­¤å€¼çš„ç»“æœï¼‰
    """
    # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
    inputs = clip_processor(text=[query_text], return_tensors="pt", padding=True)
    with torch.no_grad():
        text_features = clip_model.get_text_features(**inputs)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        text_features = text_features[0].numpy()
    
    # è®¡ç®—æ¯ä¸ªtrack_idçš„ç›¸ä¼¼åº¦
    results = []
    for track_id, feature_list in features_db.items():
        # å¯¹æ¯ä¸ªIDçš„å¤šå¼ å›¾ç‰‡ï¼Œå–æœ€å¤§ç›¸ä¼¼åº¦
        similarities = []
        for img_feature in feature_list:
            sim = np.dot(text_features, img_feature)
            similarities.append(sim)
        
        max_sim = max(similarities)
        avg_sim = np.mean(similarities)
        
        # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šåªä¿ç•™ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼çš„ç»“æœ
        if max_sim >= threshold:
            results.append({
                "track_id": track_id,
                "max_similarity": float(max_sim),
                "avg_similarity": float(avg_sim),
                "num_images": len(feature_list),
                "confidence": "high" if max_sim > 0.35 else "medium"
            })
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    results.sort(key=lambda x: x["max_similarity"], reverse=True)
    
    # è¿”å›ç»“æœï¼ˆå¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼‰
    return results[:top_k]

# æµ‹è¯•æŸ¥è¯¢
print("\nğŸ§ª æµ‹è¯•æŸ¥è¯¢ï¼š")
print("-" * 70)

test_queries = [
    "a person wearing red clothes",
    "a person wearing blue pants",
    "a person with a backpack",
    "a person wearing white shirt",
    "a person wearing purple hat",  # å¯èƒ½æ‰¾ä¸åˆ°
]

for query in test_queries:
    print(f"\næŸ¥è¯¢: \"{query}\"")
    results = search(query, top_k=3, threshold=0.25)
    
    if len(results) == 0:
        print("  âŒ æœªæ‰¾åˆ°åŒ¹é…ç»“æœï¼ˆç›¸ä¼¼åº¦å‡ä½äºé˜ˆå€¼0.25ï¼‰")
    else:
        for i, result in enumerate(results, 1):
            track_id = result["track_id"]
            similarity = result["max_similarity"]
            confidence = result["confidence"]
            emoji = "âœ…" if confidence == "high" else "âš ï¸"
            print(f"  {emoji} {i}. ID:{track_id:3d} | ç›¸ä¼¼åº¦:{similarity:.3f} ({confidence}) | "
                  f"{len(track_images[track_id])}å¼ å›¾")

# ===== ç¬¬å››æ­¥ï¼šå¯è§†åŒ–æŸ¥è¯¢ç»“æœ =====
print("\n" + "=" * 70)
print("ğŸ“Š ç¬¬å››æ­¥ï¼šå¯è§†åŒ–æŸ¥è¯¢ç»“æœ")
print("=" * 70)

def visualize_search_result(query, top_k=5, threshold=0.25):
    """å¯è§†åŒ–æŸ¥è¯¢ç»“æœ"""
    results = search(query, top_k, threshold)
    
    # åˆ›å»ºç»“æœå›¾
    result_images = []
    for result in results:
        track_id = result["track_id"]
        similarity = result["max_similarity"]
        
        # å–è¯¥IDçš„ç¬¬ä¸€å¼ å›¾
        img = track_images[track_id][0].copy()
        
        # æ·»åŠ æ–‡å­—æ ‡æ³¨
        label = f"ID:{track_id} ({similarity:.2f})"
        cv2.putText(img, label, (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        
        result_images.append(img)
    
    # æ‹¼æ¥æˆä¸€å¼ å¤§å›¾
    if result_images:
        # ç»Ÿä¸€å°ºå¯¸
        h, w = 200, 150
        resized = [cv2.resize(img, (w, h)) for img in result_images]
        
        # æ¨ªå‘æ‹¼æ¥
        result_img = np.hstack(resized)
        
        # ä¿å­˜
        query_safe = query.replace(" ", "_")[:30]
        output_path = OUTPUT_DIR / f"query_{query_safe}.jpg"
        cv2.imwrite(str(output_path), result_img)
        
        print(f"âœ… æŸ¥è¯¢ç»“æœå·²ä¿å­˜: {output_path}")
        return output_path
    
    return None

# å¯è§†åŒ–ä¸€ä¸ªæŸ¥è¯¢
query_example = "a person wearing red clothes"
print(f"\nå¯è§†åŒ–æŸ¥è¯¢: \"{query_example}\"")
visualize_search_result(query_example)

# ===== æ€»ç»“ =====
print("\n" + "=" * 70)
print("âœ… è¯­ä¹‰æŸ¥è¯¢ç³»ç»Ÿæ„å»ºå®Œæˆï¼")
print("=" * 70)

print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡:")
print(f"   ç´¢å¼•çš„IDæ•°: {len(features_db)}")
print(f"   æ€»å›¾ç‰‡æ•°: {sum(len(imgs) for imgs in track_images.values())}")
print(f"   ç‰¹å¾å‘é‡ç»´åº¦: 512")

print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
print(f"   äººç‰©å›¾ç‰‡: {crops_dir}/")
print(f"   ç‰¹å¾æ•°æ®åº“: {features_file}")
print(f"   æŸ¥è¯¢ç»“æœ: {OUTPUT_DIR}/")

print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
print(f"   1. åŠ è½½ç‰¹å¾æ•°æ®åº“")
print(f"   2. è°ƒç”¨ search(\"ä½ çš„æŸ¥è¯¢\")")
print(f"   3. è·å¾—åŒ¹é…çš„track_idåˆ—è¡¨")

print("\n" + "=" * 70)
print("ğŸ“ æ ¸å¿ƒæµç¨‹å›é¡¾:")
print("=" * 70)
print("""
1. è·Ÿè¸ªè§†é¢‘ â†’ ä¸ºæ¯ä¸ªäººåˆ†é…ID
2. æå–å›¾ç‰‡ â†’ æ¯ä¸ªIDé‡‡æ ·è‹¥å¹²å¼ 
3. CLIPç¼–ç  â†’ å›¾ç‰‡å˜æˆ512ç»´å‘é‡
4. ä¿å­˜ç‰¹å¾åº“ â†’ å»ºç«‹ç´¢å¼•
5. æŸ¥è¯¢ â†’ æ–‡å­—ç¼–ç æˆå‘é‡ â†’ åŒ¹é… â†’ è¿”å›ID
""")

print("=" * 70)
print("ğŸ¯ ç°åœ¨ä½ å¯ä»¥ç”¨è‡ªç„¶è¯­è¨€æ‰¾äººäº†ï¼")
print("   ä¾‹å¦‚: \"ç©¿çº¢è‰²è¡£æœçš„äºº\"")
print("   ä¾‹å¦‚: \"èƒŒç€èƒŒåŒ…çš„äºº\"")
print("   ä¾‹å¦‚: \"ç©¿è“è‰²è£¤å­çš„äºº\"")
print("=" * 70)

