"""
ğŸ¯ å®Œæ•´çš„è§†é¢‘è¯­ä¹‰æ£€ç´¢ç³»ç»Ÿ
Video Semantic Search System

åŠŸèƒ½æµç¨‹:
    è§†é¢‘ â†’ YOLOæ£€æµ‹ â†’ ByteTrackè·Ÿè¸ª â†’ æå–å…³é”®å¸§ â†’ VLMå±æ€§æå– â†’ å»ºç«‹æ•°æ®åº“ â†’ è¯­ä¹‰æŸ¥è¯¢

ä½œè€…: ä¸€èµ·å­¦ä¹ çš„äº§ç‰©
æ—¥æœŸ: 2025-11
"""

import cv2
import torch
import json
import time
from pathlib import Path
from collections import defaultdict
from ultralytics import YOLO
from boxmot import create_tracker
import numpy as np
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# ===== 1. é…ç½® =====
class Config:
    """ç³»ç»Ÿé…ç½®"""
    # è¾“å…¥è§†é¢‘
    VIDEO_PATH = Path("../data/snippets/debug_15s.mp4")
    
    # è¾“å‡ºç›®å½•
    OUTPUT_DIR = Path("output_full_system")
    CROPS_DIR = OUTPUT_DIR / "crops"
    
    # YOLOé…ç½®
    YOLO_MODEL = "yolov8n.pt"
    YOLO_CONF = 0.3
    YOLO_DEVICE = "mps"
    
    # è·Ÿè¸ªé…ç½®
    TRACKER_TYPE = "bytetrack"
    
    # VLMé…ç½®
    VLM_MODEL = "Qwen/Qwen2-VL-2B-Instruct"
    
    # é‡‡æ ·é…ç½®
    SAMPLE_INTERVAL = 30  # æ¯30å¸§é‡‡æ ·ä¸€æ¬¡ï¼ˆé¿å…é‡å¤ï¼‰
    MIN_TRACK_LENGTH = 10  # è‡³å°‘å‡ºç°10å¸§æ‰å¤„ç†
    
    def __init__(self):
        self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        self.CROPS_DIR.mkdir(parents=True, exist_ok=True)


# ===== 2. è§†é¢‘å¤„ç†æ¨¡å— =====
class VideoProcessor:
    """è§†é¢‘æ£€æµ‹ä¸è·Ÿè¸ª"""
    
    def __init__(self, config: Config):
        self.config = config
        print("\nğŸ”§ åˆå§‹åŒ–æ£€æµ‹ä¸è·Ÿè¸ªæ¨¡å—...")
        
        # åŠ è½½YOLO
        self.yolo = YOLO(config.YOLO_MODEL)
        
        # åŠ è½½è·Ÿè¸ªå™¨
        self.tracker = create_tracker(
            tracker_type=config.TRACKER_TYPE,
            tracker_config=None,
            reid_weights=None,
            device='cpu',
            half=False,
            per_class=False
        )
        
        print(f"   âœ… YOLO: {config.YOLO_MODEL}")
        print(f"   âœ… Tracker: {config.TRACKER_TYPE}")
    
    def process_video(self):
        """
        å¤„ç†è§†é¢‘ï¼Œæå–æ¯ä¸ªtrackçš„å…³é”®å¸§
        
        è¿”å›:
            track_data: {
                track_id: {
                    "frames": [frame_idx1, frame_idx2, ...],
                    "crops": [crop_path1, crop_path2, ...],
                    "bboxes": [(x1,y1,x2,y2), ...]
                }
            }
        """
        print(f"\nğŸ“¹ å¼€å§‹å¤„ç†è§†é¢‘: {self.config.VIDEO_PATH}")
        
        cap = cv2.VideoCapture(str(self.config.VIDEO_PATH))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        track_data = {}  # å­˜å‚¨æ¯ä¸ªtrackçš„ä¿¡æ¯
        frame_idx = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_idx += 1
            
            # è¿›åº¦æ˜¾ç¤º
            if frame_idx % 30 == 0:
                print(f"   å¤„ç†ä¸­: {frame_idx}/{total_frames} å¸§ ({frame_idx/total_frames*100:.1f}%)", end='\r')
            
            # YOLOæ£€æµ‹
            results = self.yolo.predict(
                source=frame,
                device=self.config.YOLO_DEVICE,
                conf=self.config.YOLO_CONF,
                verbose=False,
                classes=[0]  # åªæ£€æµ‹äººï¼ˆclass 0ï¼‰
            )[0]
            
            # æå–æ£€æµ‹ç»“æœ
            detections = []
            if results.boxes is not None and len(results.boxes) > 0:
                boxes = results.boxes
                for i in range(len(boxes)):
                    x1, y1, x2, y2 = boxes.xyxy[i].cpu().numpy()
                    conf = float(boxes.conf[i])
                    cls = int(boxes.cls[i])
                    detections.append([x1, y1, x2, y2, conf, cls])
            
            # è·Ÿè¸ª
            if len(detections) > 0:
                detections = np.array(detections)
                tracks = self.tracker.update(detections, frame)
                
                if tracks.size > 0:
                    for track in tracks:
                        x1, y1, x2, y2 = map(int, track[:4])
                        track_id = int(track[4])
                        
                        # åˆå§‹åŒ–trackè®°å½•
                        if track_id not in track_data:
                            track_data[track_id] = {
                                "frames": [],
                                "crops": [],
                                "bboxes": []
                            }
                        
                        track_data[track_id]["frames"].append(frame_idx)
                        track_data[track_id]["bboxes"].append((x1, y1, x2, y2))
                        
                        # é‡‡æ ·å…³é”®å¸§ï¼ˆæ¯Nå¸§ä¿å­˜ä¸€æ¬¡ï¼‰
                        if len(track_data[track_id]["frames"]) % self.config.SAMPLE_INTERVAL == 1:
                            # è£å‰ªäººç‰©å›¾ç‰‡
                            crop = frame[y1:y2, x1:x2]
                            if crop.size > 0:
                                crop_path = self.config.CROPS_DIR / f"id{track_id:03d}_frame{frame_idx:05d}.jpg"
                                cv2.imwrite(str(crop_path), crop)
                                track_data[track_id]["crops"].append(str(crop_path))
        
        cap.release()
        print(f"\n   âœ… å¤„ç†å®Œæˆ: {total_frames} å¸§")
        
        # è¿‡æ»¤çŸ­track
        filtered_data = {
            tid: data for tid, data in track_data.items()
            if len(data["frames"]) >= self.config.MIN_TRACK_LENGTH
        }
        
        print(f"   ğŸ“Š æ€»å…±æ£€æµ‹åˆ° {len(track_data)} ä¸ªç›®æ ‡")
        print(f"   ğŸ“Š è¿‡æ»¤åå‰©ä½™ {len(filtered_data)} ä¸ªæœ‰æ•ˆç›®æ ‡")
        
        return filtered_data

    def render_highlight_video(self, track_data, target_track_ids, output_path, label_text="target"):
        """å°†æ»¡è¶³æ¡ä»¶çš„trackåœ¨åŸè§†é¢‘ä¸Šé«˜äº®å¹¶å¯¼å‡º"""
        if not target_track_ids:
            print("   âš ï¸  æ²¡æœ‰ç›®æ ‡éœ€è¦å¯è§†åŒ–ï¼Œè·³è¿‡è§†é¢‘å¯¼å‡º")
            return

        target_ids = set(target_track_ids)
        frame_map = defaultdict(list)
        for tid in target_ids:
            data = track_data.get(tid)
            if not data:
                continue
            for frame_idx, bbox in zip(data["frames"], data["bboxes"]):
                frame_map[frame_idx].append((tid, bbox))

        cap = cv2.VideoCapture(str(self.config.VIDEO_PATH))
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))

        frame_idx = 0
        highlight_color = (0, 0, 255)  # çº¢è‰²è¾¹æ¡†

        print(f"\nğŸ“¼ å¯¼å‡ºé«˜äº®è§†é¢‘: {output_path}")

        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame_idx += 1

            tracks_this_frame = frame_map.get(frame_idx, [])
            for tid, bbox in tracks_this_frame:
                x1, y1, x2, y2 = bbox
                cv2.rectangle(frame, (x1, y1), (x2, y2), highlight_color, 3)
                cv2.putText(
                    frame,
                    f"ID:{tid}",
                    (x1, max(30, y1 - 10)),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1.0,
                    highlight_color,
                    2
                )

            if tracks_this_frame:
                cv2.putText(
                    frame,
                    f"Tracking {label_text}",
                    (20, 60),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1.2,
                    (0, 0, 255),
                    3
                )

            out.write(frame)

        cap.release()
        out.release()
        print("   âœ… å·²ç”Ÿæˆé«˜äº®è§†é¢‘")


# ===== 3. VLMå±æ€§æå–æ¨¡å— =====
class AttributeExtractor:
    """ä½¿ç”¨VLMæå–æ¯ä¸ªtrackçš„å±æ€§"""
    
    def __init__(self, config: Config):
        self.config = config
        print("\nğŸ”§ åˆå§‹åŒ–VLMæ¨¡å—...")
        
        # æ£€æµ‹è®¾å¤‡
        if torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"
        
        # åŠ è½½æ¨¡å‹
        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
            config.VLM_MODEL,
            torch_dtype=torch.float16 if self.device == "mps" else torch.float32,
            device_map=self.device
        )
        self.processor = AutoProcessor.from_pretrained(config.VLM_MODEL)
        
        print(f"   âœ… æ¨¡å‹: {config.VLM_MODEL}")
        print(f"   âœ… è®¾å¤‡: {self.device.upper()}")
    
    def query_image(self, image_path, question):
        """å¯¹å•å¼ å›¾ç‰‡æé—®"""
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": str(image_path)},
                    {"type": "text", "text": question}
                ]
            }
        ]
        
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            generated_ids = self.model.generate(**inputs, max_new_tokens=50)
        
        generated_ids_trimmed = [
            out_ids[len(in_ids):]
            for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)
        ]
        
        answer = self.processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        return answer.strip()
    
    def extract_attributes(self, track_data):
        """
        ä¸ºæ¯ä¸ªtrackæå–å±æ€§
        
        å‚æ•°:
            track_data: VideoProcessorè¿”å›çš„è·Ÿè¸ªæ•°æ®
        
        è¿”å›:
            attributes: {
                track_id: {
                    "color": "red",
                    "has_backpack": True,
                    "gender": "male",
                    ...
                }
            }
        """
        print(f"\nğŸ” å¼€å§‹æå–å±æ€§ ({len(track_data)} ä¸ªç›®æ ‡)...")
        
        attributes = {}
        
        # å®šä¹‰è¦é—®çš„é—®é¢˜
        questions = [
            ("color", "What is the main color of this person's clothing? Answer with one color word only."),
            ("has_backpack", "Is this person carrying a backpack? Answer yes or no."),
            ("upper_color", "What color is this person's upper body clothing? Answer with one color word only."),
        ]
        
        for idx, (track_id, data) in enumerate(track_data.items(), 1):
            print(f"\n[{idx}/{len(track_data)}] å¤„ç† Track ID: {track_id}")
            
            # åªç”¨ç¬¬ä¸€å¼ cropï¼ˆä»£è¡¨æ€§å›¾ç‰‡ï¼‰
            if not data["crops"]:
                print(f"   âš ï¸  æ²¡æœ‰å¯ç”¨å›¾ç‰‡ï¼Œè·³è¿‡")
                continue
            
            crop_path = data["crops"][0]
            print(f"   ğŸ“· ä½¿ç”¨å›¾ç‰‡: {Path(crop_path).name}")
            
            # æå–å±æ€§
            attrs = {}
            for attr_name, question in questions:
                try:
                    answer = self.query_image(crop_path, question)
                    
                    # å¤„ç†yes/noé—®é¢˜
                    if attr_name.startswith("has_"):
                        attrs[attr_name] = any(
                            word in answer.lower() 
                            for word in ['yes', 'yeah', 'yep', 'true']
                        )
                    else:
                        attrs[attr_name] = answer.lower().strip()
                    
                    print(f"   âœ… {attr_name}: {attrs[attr_name]}")
                
                except Exception as e:
                    print(f"   âŒ {attr_name}: æå–å¤±è´¥ ({str(e)})")
                    attrs[attr_name] = None
            
            attributes[track_id] = attrs
        
        return attributes


# ===== 4. è¯­ä¹‰æŸ¥è¯¢æ¨¡å— =====
class SemanticSearchEngine:
    """è¯­ä¹‰æŸ¥è¯¢å¼•æ“"""
    
    def __init__(self, track_data, attributes):
        self.track_data = track_data
        self.attributes = attributes
        
        print("\nğŸ”§ åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“...")
        print(f"   âœ… åŠ è½½ {len(attributes)} ä¸ªç›®æ ‡çš„å±æ€§æ•°æ®")
    
    def search(self, query_type, query_value):
        """
        è¯­ä¹‰æŸ¥è¯¢
        
        å‚æ•°:
            query_type: æŸ¥è¯¢ç±»å‹ï¼ˆcolor, has_backpack, upper_colorç­‰ï¼‰
            query_value: æŸ¥è¯¢å€¼ï¼ˆå¦‚"red", Trueç­‰ï¼‰
        
        è¿”å›:
            åŒ¹é…çš„track_idåˆ—è¡¨
        """
        print(f"\nğŸ” æŸ¥è¯¢: {query_type} = {query_value}")
        
        results = []
        for track_id, attrs in self.attributes.items():
            if query_type not in attrs:
                continue
            
            attr_value = attrs[query_type]
            
            # å¸ƒå°”å€¼ç›´æ¥æ¯”è¾ƒ
            if isinstance(query_value, bool):
                if attr_value == query_value:
                    results.append(track_id)
            # å­—ç¬¦ä¸²æ¨¡ç³ŠåŒ¹é…
            elif isinstance(query_value, str):
                if query_value.lower() in str(attr_value).lower():
                    results.append(track_id)
        
        print(f"   ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ç»“æœ: {results}")
        return results
    
    def complex_search(self, conditions):
        """
        å¤åˆæ¡ä»¶æŸ¥è¯¢
        
        å‚æ•°:
            conditions: [(query_type, query_value), ...]
        
        ç¤ºä¾‹:
            conditions = [("color", "red"), ("has_backpack", True)]
        """
        print(f"\nğŸ” å¤åˆæŸ¥è¯¢: {conditions}")
        
        # åˆå§‹ä¸ºæ‰€æœ‰track_id
        result_set = set(self.attributes.keys())
        
        # é€ä¸ªæ¡ä»¶è¿‡æ»¤
        for query_type, query_value in conditions:
            matched = set(self.search(query_type, query_value))
            result_set = result_set.intersection(matched)
        
        results = list(result_set)
        print(f"\n   ğŸ¯ æœ€ç»ˆåŒ¹é…: {len(results)} ä¸ªç»“æœ {results}")
        return results
    
    def visualize_results(self, track_ids, output_path):
        """å¯è§†åŒ–æŸ¥è¯¢ç»“æœ"""
        if not track_ids:
            print("   âš ï¸  æ²¡æœ‰ç»“æœå¯è§†åŒ–")
            return
        
        print(f"\nğŸ“Š ç”Ÿæˆç»“æœå¯è§†åŒ–...")
        
        # æ”¶é›†å›¾ç‰‡
        images = []
        for tid in track_ids[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            if tid in self.track_data and self.track_data[tid]["crops"]:
                crop_path = self.track_data[tid]["crops"][0]
                img = cv2.imread(crop_path)
                if img is not None:
                    # æ·»åŠ IDæ ‡ç­¾
                    cv2.putText(
                        img, f"ID:{tid}",
                        (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        1, (0, 255, 0), 2
                    )
                    images.append(img)
        
        if not images:
            print("   âš ï¸  æ²¡æœ‰å¯ç”¨å›¾ç‰‡")
            return
        
        # æ‹¼æ¥å›¾ç‰‡
        if len(images) == 1:
            result = images[0]
        else:
            # æ°´å¹³æ‹¼æ¥
            max_height = max(img.shape[0] for img in images)
            resized = []
            for img in images:
                h, w = img.shape[:2]
                scale = max_height / h
                new_w = int(w * scale)
                resized.append(cv2.resize(img, (new_w, max_height)))
            result = np.hstack(resized)
        
        cv2.imwrite(str(output_path), result)
        print(f"   âœ… ä¿å­˜åˆ°: {output_path}")


# ===== 5. ä¸»ç³»ç»Ÿ =====
class VideoSemanticSearchSystem:
    """å®Œæ•´çš„è§†é¢‘è¯­ä¹‰æ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(self, config: Config):
        self.config = config
        
        print("=" * 70)
        print("ğŸš€ è§†é¢‘è¯­ä¹‰æ£€ç´¢ç³»ç»Ÿ")
        print("=" * 70)
        
        # åˆå§‹åŒ–æ¨¡å—
        self.video_processor = VideoProcessor(config)
        self.attribute_extractor = AttributeExtractor(config)
        
        self.track_data = None
        self.attributes = None
        self.search_engine = None
    
    def build_index(self):
        """æ„å»ºç´¢å¼•ï¼ˆå¤„ç†è§†é¢‘+æå–å±æ€§ï¼‰"""
        print("\n" + "=" * 70)
        print("ğŸ“¦ é˜¶æ®µ1: æ„å»ºç´¢å¼•")
        print("=" * 70)
        
        # Step 1: å¤„ç†è§†é¢‘
        self.track_data = self.video_processor.process_video()
        
        # Step 2: æå–å±æ€§
        self.attributes = self.attribute_extractor.extract_attributes(self.track_data)
        
        # Step 3: ä¿å­˜æ•°æ®åº“
        db_path = self.config.OUTPUT_DIR / "attribute_database.json"
        with open(db_path, 'w', encoding='utf-8') as f:
            json.dump({
                "track_data": {
                    str(k): {
                        "frames": v["frames"],
                        "crops": v["crops"],
                        "num_bboxes": len(v["bboxes"])
                    }
                    for k, v in self.track_data.items()
                },
                "attributes": {str(k): v for k, v in self.attributes.items()}
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ æ•°æ®åº“å·²ä¿å­˜: {db_path}")
        
        # Step 4: åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“
        self.search_engine = SemanticSearchEngine(self.track_data, self.attributes)
        
        print("\nâœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")
    
    def search(self, query_description, conditions):
        """
        æ‰§è¡ŒæŸ¥è¯¢
        
        å‚æ•°:
            query_description: æŸ¥è¯¢æè¿°ï¼ˆç”¨äºæ–‡ä»¶åï¼‰
            conditions: [(query_type, query_value), ...]
        """
        if self.search_engine is None:
            print("âŒ è¯·å…ˆè¿è¡Œ build_index()")
            return []
        
        print("\n" + "=" * 70)
        print("ğŸ” é˜¶æ®µ2: è¯­ä¹‰æŸ¥è¯¢")
        print("=" * 70)
        
        results = self.search_engine.complex_search(conditions)
        
        # å¯è§†åŒ–
        if results:
            output_path = self.config.OUTPUT_DIR / f"result_{query_description}.jpg"
            self.search_engine.visualize_results(results, output_path)

            # ç”Ÿæˆé«˜äº®è§†é¢‘ï¼Œå±•ç¤ºâ€œå¸®æˆ‘è·Ÿè¸ªè¿™äº›äººâ€èƒ½åŠ›
            safe_name = query_description.replace("/", "_").replace(" ", "_")
            tracking_video = self.config.OUTPUT_DIR / f"tracking_{safe_name}.mp4"
            self.video_processor.render_highlight_video(
                self.track_data,
                results,
                tracking_video,
                label_text=query_description
            )
        
        return results


# ===== 6. ä¸»å‡½æ•° =====
def main():
    """ä¸»æµç¨‹"""
    
    # é…ç½®
    config = Config()
    
    # åˆ›å»ºç³»ç»Ÿ
    system = VideoSemanticSearchSystem(config)
    
    # æ„å»ºç´¢å¼•
    system.build_index()
    
    # æ‰§è¡ŒæŸ¥è¯¢
    print("\n" + "=" * 70)
    print("ğŸ¯ æµ‹è¯•æŸ¥è¯¢")
    print("=" * 70)
    
    # æŸ¥è¯¢1: ç©¿çº¢è‰²è¡£æœçš„äºº
    system.search(
        query_description="ç©¿çº¢è‰²è¡£æœçš„äºº",
        conditions=[("color", "red")]
    )
    
    # æŸ¥è¯¢2: èƒŒèƒŒåŒ…çš„äºº
    system.search(
        query_description="èƒŒèƒŒåŒ…çš„äºº",
        conditions=[("has_backpack", True)]
    )
    
    # æŸ¥è¯¢3: å¤åˆæŸ¥è¯¢ï¼ˆç©¿è“è‰²è¡£æœä¸”èƒŒèƒŒåŒ…ï¼‰
    system.search(
        query_description="ç©¿è“è‰²è¡£æœä¸”èƒŒèƒŒåŒ…",
        conditions=[("color", "blue"), ("has_backpack", True)]
    )
    
    print("\n" + "=" * 70)
    print("âœ… ç³»ç»Ÿè¿è¡Œå®Œæˆï¼")
    print("=" * 70)


if __name__ == '__main__':
    main()
