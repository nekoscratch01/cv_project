# ä½¿ç”¨vLLMåŠ é€Ÿçš„VLMè¯­ä¹‰æŸ¥è¯¢ç³»ç»Ÿ
# é€‚åˆM4ï¼šä½¿ç”¨Qwen2-VL-2Bï¼ˆå°å‹å¤šæ¨¡æ€æ¨¡å‹ï¼‰

import cv2
import json
import numpy as np
from pathlib import Path
from PIL import Image
import time

print("=" * 70)
print("ğŸš€ vLLMåŠ é€Ÿçš„VLMæŸ¥è¯¢ç³»ç»Ÿ")
print("=" * 70)

OUTPUT_DIR = Path("output_semantic")
crops_dir = OUTPUT_DIR / "crops"

# ===== 1. æ£€æŸ¥ä¾èµ– =====
print("\nğŸ“¦ æ£€æŸ¥ä¾èµ–...")

try:
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
    from qwen_vl_utils import process_vision_info
    import torch
    print("âœ… transformerså’ŒQwen-VLå·¥å…·å·²å®‰è£…")
except ImportError as e:
    print(f"âŒ ä¾èµ–æœªå®‰è£…: {e}")
    print("\nğŸ’¡ å®‰è£…æ–¹æ³•ï¼š")
    print("   pip install transformers qwen-vl-utils torch")
    exit()

# æ£€æŸ¥MPSå¯ç”¨æ€§
if torch.backends.mps.is_available():
    device = "mps"
    print("âœ… æ£€æµ‹åˆ°MPS (Apple Silicon)ï¼Œå°†ä½¿ç”¨GPUåŠ é€Ÿ")
else:
    device = "cpu"
    print("âš ï¸  MPSä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")

# ===== 2. é€‰æ‹©æ¨¡å‹ =====
print("\nğŸ¤– æ¨¡å‹é€‰æ‹©...")

# æ¨èæ¨¡å‹ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
MODELS = {
    "qwen2-vl-2b": {
        "model_id": "Qwen/Qwen2-VL-2B-Instruct",
        "size": "2B",
        "memory": "~5GB",
        "speed": "å¿«",
        "accuracy": "ä¸­é«˜",
        "m4_compatible": True,
        "description": "é˜¿é‡Œé€šä¹‰åƒé—®2-VLï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆæœ"
    },
    "llava-v1.6-vicuna-7b": {
        "model_id": "llava-hf/llava-v1.6-vicuna-7b-hf",
        "size": "7B",
        "memory": "~14GB",
        "speed": "ä¸­",
        "accuracy": "é«˜",
        "m4_compatible": False,  # è¶…è¿‡16GBé™åˆ¶
        "description": "LLaVA 1.6ï¼Œæ•ˆæœå¥½ä½†å†…å­˜éœ€æ±‚å¤§"
    },
    "moondream2": {
        "model_id": "vikhyatk/moondream2",
        "size": "1.6B",
        "memory": "~4GB",
        "speed": "å¿«",
        "accuracy": "ä¸­",
        "m4_compatible": True,
        "description": "è½»é‡çº§VLMï¼Œä¸“ä¸ºå°è®¾å¤‡ä¼˜åŒ–"
    },
}

# é€‰æ‹©æ¨¡å‹ï¼ˆM4æ¨èqwen2-vl-2bï¼‰
selected_model = "qwen2-vl-2b"
model_info = MODELS[selected_model]

print(f"\nğŸ“Œ ä½¿ç”¨æ¨¡å‹: {model_info['model_id']}")
print(f"   å¤§å°: {model_info['size']}")
print(f"   å†…å­˜éœ€æ±‚: {model_info['memory']}")
print(f"   é€Ÿåº¦: {model_info['speed']}")
print(f"   æè¿°: {model_info['description']}")

# ===== 3. åŠ è½½Qwen2-VLæ¨¡å‹ï¼ˆtransformers + MPSï¼‰=====
print("\nğŸ”„ åŠ è½½Qwen2-VLæ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½ï¼‰...")
print("   è¿™å¯èƒ½éœ€è¦5-10åˆ†é’Ÿ...")

start_time = time.time()

print("åŠ è½½æ¨¡å‹...")
model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_info['model_id'],
    torch_dtype=torch.float16 if device == "mps" else torch.float32,
    device_map=device  # M4ä½¿ç”¨MPSåŠ é€Ÿ
)
processor = AutoProcessor.from_pretrained(model_info['model_id'])

load_time = time.time() - start_time
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.1f}ç§’)")
print(f"   è®¾å¤‡: {device.upper()}")

# ===== 4. å®šä¹‰æŸ¥è¯¢å‡½æ•° =====
def query_image_vlm(image_path, question):
    """ä½¿ç”¨Qwen2-VLæŸ¥è¯¢å›¾ç‰‡"""
    # æ„é€ æ¶ˆæ¯ï¼ˆæ³¨æ„ï¼šå¿…é¡»è½¬ä¸ºå­—ç¬¦ä¸²è·¯å¾„ï¼ï¼‰
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": str(image_path)},  # âœ… è½¬ä¸ºå­—ç¬¦ä¸²
                {"type": "text", "text": question}
            ]
        }
    ]
    
    # å‡†å¤‡è¾“å…¥
    text = processor.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    image_inputs, video_inputs = process_vision_info(messages)
    
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt"
    )
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # ç”Ÿæˆ
    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=50)
    
    # ä¿®å‰ªç”Ÿæˆçš„IDï¼ˆç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
    generated_ids_trimmed = [
        out_ids[len(in_ids):] 
        for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)
    ]
    
    # è§£ç 
    answer = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]
    
    return answer.strip()

# ===== 5. ä¸»ç¨‹åºï¼ˆæ·»åŠ ä¿æŠ¤é¿å…multiprocessingé—®é¢˜ï¼‰=====
def main():
    """ä¸»å‡½æ•°"""
    # æµ‹è¯•å•å¼ å›¾ç‰‡
    print("\n" + "=" * 70)
    print("ğŸ§ª æµ‹è¯•ï¼šVLMç†è§£èƒ½åŠ›")
    print("=" * 70)

    test_images = sorted(crops_dir.glob("*.jpg"))[:3]

    if len(test_images) == 0:
        print("âŒ æ‰¾ä¸åˆ°å›¾ç‰‡ï¼Œè¯·å…ˆè¿è¡Œ semantic_search_complete.py")
        return

    print(f"\næµ‹è¯• {len(test_images)} å¼ å›¾ç‰‡:\n")

    for img_path in test_images:
        track_id = int(img_path.stem.split('_')[0][2:])
        
        print(f"ğŸ“· {img_path.name} (ID:{track_id})")
        print("-" * 70)
        
        # é—®å¤šä¸ªé—®é¢˜
        questions = [
            "What color is this person's clothing?",
            "Describe this person's appearance briefly.",
            "Is this person carrying anything?",
        ]
        
        for question in questions:
            start = time.time()
            answer = query_image_vlm(img_path, question)
            elapsed = time.time() - start
            
            print(f"  Q: {question}")
            print(f"  A: {answer}")
            print(f"  â±  {elapsed:.2f}ç§’")
        print()
    
    # æ™ºèƒ½æŸ¥è¯¢
    print("\n" + "=" * 70)
    print("ğŸ” æ™ºèƒ½æŸ¥è¯¢ï¼šæ‰¾ç¬¦åˆæè¿°çš„äºº")
    print("=" * 70)
    
    # ğŸ”§ æ”¹è¿›ï¼šç›´æ¥ä¼ å…¥å®Œæ•´çš„yes/noé—®é¢˜
    test_queries = [
        ("ç©¿çº¢è‰²è¡£æœçš„äºº", "Is this person wearing red clothes?"),
        ("ç©¿è“è‰²è¡£æœçš„äºº", "Is this person wearing blue clothes?"),
        ("èƒŒèƒŒåŒ…çš„äºº", "Is this person carrying a backpack?"),
    ]

    for description, question in test_queries:
        smart_search_vlm_v2(description, question, top_k=3)
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“Š VLM vs CLIP å¯¹æ¯”")
    print("=" * 70)

    print("""
| ç»´åº¦ | CLIP | VLM (Qwen2-VL) |
|------|------|----------------|
| **å‡†ç¡®ç‡ï¼ˆé¢œè‰²ï¼‰** | â­â­ 20-30% | â­â­â­â­ 70-85% |
| **é€Ÿåº¦** | âš¡ 0.1ç§’/å¼  | ğŸŒ 1-2ç§’/å¼  |
| **å†…å­˜** | 2GB | 5GB |
| **èƒ½åŠ›** | å‘é‡åŒ¹é… | çœŸæ­£ç†è§£ |
| **çµæ´»æ€§** | å›ºå®šæŸ¥è¯¢ | ä»»æ„é—®ç­” |
| **M4å¯è¡Œæ€§** | âœ… å®Œç¾ | âœ… è‰¯å¥½ï¼ˆMPSåŠ é€Ÿï¼‰ |

ğŸ“ **æ ¸å¿ƒåŒºåˆ«ï¼š**

CLIPï¼ˆå›¾åƒ-æ–‡æœ¬åŒ¹é…ï¼‰ï¼š
  å›¾ç‰‡ â†’ [0.1, 0.3, ...] å‘é‡
  æ–‡å­— â†’ [0.12, 0.28, ...] å‘é‡
  è®¡ç®—è·ç¦» â†’ 0.28 ç›¸ä¼¼åº¦
  
  é—®é¢˜ï¼šä¸"ç†è§£"å›¾ç‰‡å†…å®¹ï¼Œåªæ˜¯å‘é‡è·ç¦»

VLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰ï¼š
  å›¾ç‰‡ â†’ è§†è§‰ç†è§£ â†’ "ä¸€ä¸ªç©¿çº¢è‰²å¤¹å…‹çš„äºº"
  é—®é¢˜ â†’ "ç©¿ä»€ä¹ˆé¢œè‰²ï¼Ÿ"
  æ¨ç† â†’ "çº¢è‰²"
  
  ä¼˜åŠ¿ï¼šçœŸæ­£"çœ‹æ‡‚"äº†å›¾ç‰‡

ğŸ’¡ **ä½¿ç”¨åœºæ™¯å»ºè®®ï¼š**

1. **å¿«é€ŸåŸå‹/å®æ—¶å¤„ç†** â†’ CLIP
   - é€Ÿåº¦å¿«ï¼ˆ0.1s vs 2sï¼‰
   - å†…å­˜å°ï¼ˆ2GB vs 5GBï¼‰
   - ä½œä¸ºåˆç­›å·¥å…·

2. **é«˜å‡†ç¡®ç‡/ç¦»çº¿åˆ†æ** â†’ VLM
   - å‡†ç¡®ç‡é«˜3-4å€
   - èƒ½å›ç­”å¤æ‚é—®é¢˜
   - é€‚åˆæœ€ç»ˆç¡®è®¤

3. **ç»„åˆä½¿ç”¨**ï¼ˆæœ€ä½³å®è·µï¼‰ï¼š
   æ­¥éª¤1: CLIPå¿«é€Ÿç­›é€‰ï¼ˆ35ä¸ªâ†’10ä¸ªï¼‰
   æ­¥éª¤2: VLMç²¾ç¡®ç¡®è®¤ï¼ˆ10ä¸ªâ†’3ä¸ªï¼‰
   æ­¥éª¤3: äººå·¥æœ€ç»ˆéªŒè¯
""")

    print("\n" + "=" * 70)
    print("âœ… VLMç³»ç»Ÿæ„å»ºå®Œæˆï¼")
    print("=" * 70)

    print(f"\nğŸ’¡ ä½¿ç”¨transformers + MPSåŠ é€Ÿ")
    print(f"   - è®¾å¤‡: {device.upper()}")
    print(f"   - æ¨¡å‹: Qwen2-VL-2B")
    print(f"   - é€‚åˆM4èŠ¯ç‰‡")

# ===== 7. è¾…åŠ©å‡½æ•° =====
def smart_search_vlm_v2(query_description, yes_no_question, top_k=5):
    """
    ğŸ”§ æ”¹è¿›ç‰ˆï¼šä½¿ç”¨VLMè¿›è¡Œæ™ºèƒ½æŸ¥è¯¢ï¼ˆç›´æ¥é—®yes/noé—®é¢˜ï¼‰
    
    å‚æ•°:
        query_description: æŸ¥è¯¢æè¿°ï¼ˆå¦‚"ç©¿çº¢è‰²è¡£æœçš„äºº"ï¼‰
        yes_no_question: å®Œæ•´çš„yes/noé—®é¢˜ï¼ˆå¦‚"Is this person wearing red clothes?"ï¼‰
        top_k: è¿”å›å‰kä¸ªç»“æœ
    
    ç¤ºä¾‹:
        smart_search_vlm_v2("ç©¿çº¢è‰²è¡£æœçš„äºº", "Is this person wearing red clothes?", top_k=3)
    """
    print(f"\nğŸ¯ æŸ¥æ‰¾: {query_description}")
    print(f"   é—®é¢˜: {yes_no_question}")
    print("-" * 70)
    
    results = []
    processed_ids = set()
    
    # éå†æ‰€æœ‰IDï¼ˆæ¯ä¸ªIDåªå–ç¬¬ä¸€å¼ å›¾ï¼‰
    all_images = sorted(crops_dir.glob("*.jpg"))
    total_ids = len(set(int(p.stem.split('_')[0][2:]) for p in all_images))
    
    print(f"   å¤„ç† {total_ids} ä¸ªID...\n")
    
    for img_path in all_images:
        track_id = int(img_path.stem.split('_')[0][2:])
        
        if track_id in processed_ids:
            continue
        processed_ids.add(track_id)
        
        # ç›´æ¥é—®yes/noé—®é¢˜
        answer = query_image_vlm(img_path, yes_no_question + " Answer yes or no.")
        answer_lower = answer.lower().strip()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè‚¯å®šå›ç­”
        matched = any(pos in answer_lower for pos in ['yes', 'yeah', 'yep', 'correct', 'true'])
        
        if matched:
            results.append({
                "track_id": track_id,
                "answer": answer,
                "image": str(img_path),
            })
            print(f"   âœ… ID:{track_id:3d} â†’ {answer}")
        else:
            print(f"   âŒ ID:{track_id:3d} â†’ {answer}")
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 70)
    print(f"ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ç»“æœ")
    print("=" * 70)
    
    if not results:
        print("   æœªæ‰¾åˆ°åŒ¹é…çš„äºº\n")
        return []
    
    # ä¿å­˜ç»“æœå›¾ç‰‡
    for i, result in enumerate(results[:top_k], 1):
        track_id = result['track_id']
        print(f"{i}. Track ID: {track_id}")
        print(f"   å›ç­”: {result['answer']}")
        print(f"   å›¾ç‰‡: {Path(result['image']).name}\n")
    
    # ç”Ÿæˆç»“æœå›¾ï¼ˆç»„åˆå‰top_kä¸ªï¼‰
    output_file = output_dir / f"vlm_{query_description}.jpg"
    create_result_visualization(results[:top_k], output_file)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}\n")
    
    return results[:top_k]

def smart_search_vlm(query_description, match_keywords, top_k=5):
    """
    ä½¿ç”¨VLMè¿›è¡Œæ™ºèƒ½æŸ¥è¯¢
    
    å‚æ•°:
        query_description: æŸ¥è¯¢æè¿°ï¼ˆå¦‚"ç©¿çº¢è‰²è¡£æœçš„äºº"ï¼‰
        match_keywords: ç”¨äºæ„å»ºé—®é¢˜ï¼ˆå¦‚["red"]åˆ™é—®"wearing red clothes"ï¼‰
        top_k: è¿”å›å‰kä¸ªç»“æœ
    """
    print(f"\nğŸ¯ æŸ¥æ‰¾: {query_description}")
    print(f"   ç›®æ ‡ç‰¹å¾: {match_keywords[0]}")
    print("-" * 70)
    
    results = []
    processed_ids = set()
    
    # éå†æ‰€æœ‰IDï¼ˆæ¯ä¸ªIDåªå–ç¬¬ä¸€å¼ å›¾ï¼‰
    all_images = sorted(crops_dir.glob("*.jpg"))
    total_ids = len(set(int(p.stem.split('_')[0][2:]) for p in all_images))
    
    print(f"   å¤„ç† {total_ids} ä¸ªID...\n")
    
    for img_path in all_images:
        track_id = int(img_path.stem.split('_')[0][2:])
        
        if track_id in processed_ids:
            continue
        processed_ids.add(track_id)
        
        # ğŸ”§ æ”¹è¿›ï¼šç›´æ¥é—®ç›®æ ‡é—®é¢˜ï¼Œè€Œä¸æ˜¯å¼€æ”¾å¼æé—®
        primary_keyword = match_keywords[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå…³é”®è¯
        question = f"Is this person wearing {primary_keyword} clothes? Answer yes or no."
        answer = query_image_vlm(img_path, question)
        answer_lower = answer.lower().strip()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè‚¯å®šå›ç­”
        matched = any(pos in answer_lower for pos in ['yes', 'yeah', 'yep', 'correct', 'true'])
        
        if matched:
            results.append({
                "track_id": track_id,
                "answer": answer,
                "image_path": img_path
            })
            print(f"  âœ… ID:{track_id:3d} | å›ç­”: \"{answer}\" | åŒ¹é…ï¼")
        
        # è¿›åº¦
        if len(processed_ids) % 5 == 0:
            print(f"     è¿›åº¦: {len(processed_ids)}/{total_ids}")
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…")
    
    # å¯è§†åŒ–ç»“æœ
    if results:
        visualize_vlm_results(query_description, results[:top_k])
    
    return results[:top_k]

def visualize_vlm_results(query, results):
    """å¯è§†åŒ–VLMæŸ¥è¯¢ç»“æœ"""
    print(f"\nğŸ–¼ï¸  ç”Ÿæˆç»“æœå›¾...")
    
    result_images = []
    for result in results:
        img = cv2.imread(str(result['image_path']))
        if img is not None:
            # æ·»åŠ æ ‡æ³¨
            track_id = result['track_id']
            answer = result['answer'][:20]  # æˆªæ–­é•¿ç­”æ¡ˆ
            
            cv2.putText(img, f"ID:{track_id}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            cv2.putText(img, answer, (10, img.shape[0] - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)
            
            result_images.append(img)
    
    if result_images:
        h, w = 200, 150
        resized = [cv2.resize(img, (w, h)) for img in result_images]
        result_img = np.hstack(resized)
        
        query_safe = query.replace(" ", "_")[:30]
        output_path = OUTPUT_DIR / f"vlm_{query_safe}.jpg"
        cv2.imwrite(str(output_path), result_img)
        print(f"âœ… ç»“æœå·²ä¿å­˜: {output_path}")

# ===== 8. ç¨‹åºå…¥å£ =====
if __name__ == '__main__':
    main()

