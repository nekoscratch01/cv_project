"""
ğŸ¯ è§†é¢‘ä¸­æ‰¾äººç³»ç»Ÿ (Find Person in Video)
Video Person Search System

çœŸå®åœºæ™¯: ç›‘æ§è§†é¢‘ä¸­æ ¹æ®æè¿°æ‰¾äºº
ä¾‹å¦‚: "æ‰¾å‡ºç©¿çº¢è‰²è¡£æœçš„äºº" / "æ‰¾å‡ºèƒŒèƒŒåŒ…çš„äºº"

ä½œè€…: ä¸€èµ·å­¦ä¹ çš„äº§ç‰©
æ—¥æœŸ: 2025-11
"""

import cv2
import torch
import json
import time
from pathlib import Path
from ultralytics import YOLO
from boxmot import create_tracker
import numpy as np
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# ===== é…ç½® =====
VIDEO_PATH = Path("../data/snippets/debug_15s.mp4")
OUTPUT_DIR = Path("output_search_results")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ===== 1. è§†é¢‘å¤„ç†ï¼šæ£€æµ‹+è·Ÿè¸ª+è£å‰ª =====
def process_video_and_extract_people(video_path, output_crops_dir):
    """
    å¤„ç†è§†é¢‘ï¼Œæå–æ‰€æœ‰äººç‰©
    
    è¿”å›:
        people: {
            track_id: {
                "image": "crops/id001.jpg",  # ä»£è¡¨æ€§å›¾ç‰‡
                "frames": [1, 2, 3, ...],    # å‡ºç°çš„å¸§å·
                "first_bbox": (x1, y1, x2, y2)  # ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®
            }
        }
    """
    print("\n" + "=" * 70)
    print("ğŸ“¹ é˜¶æ®µ1: å¤„ç†è§†é¢‘ï¼Œæå–æ‰€æœ‰äººç‰©")
    print("=" * 70)
    
    output_crops_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ”§ åŠ è½½YOLOå’Œè·Ÿè¸ªå™¨...")
    yolo = YOLO("yolov8n.pt")
    tracker = create_tracker(
        tracker_type='bytetrack',
        tracker_config=None,
        reid_weights=None,
        device='cpu',
        half=False,
        per_class=False
    )
    
    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    print(f"   è§†é¢‘: {video_path.name}")
    print(f"   æ€»å¸§æ•°: {total_frames}")
    print(f"   å¸§ç‡: {fps:.2f} FPS")
    
    people = {}  # å­˜å‚¨æ¯ä¸ªäººçš„ä¿¡æ¯
    frame_idx = 0
    
    print("\nğŸ”„ å¼€å§‹å¤„ç†...")
    start_time = time.time()
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_idx += 1
        
        if frame_idx % 30 == 0:
            elapsed = time.time() - start_time
            fps_processing = frame_idx / elapsed
            eta = (total_frames - frame_idx) / fps_processing
            print(f"   è¿›åº¦: {frame_idx}/{total_frames} ({frame_idx/total_frames*100:.1f}%) | "
                  f"é€Ÿåº¦: {fps_processing:.1f} FPS | ETA: {eta:.0f}ç§’", end='\r')
        
        # YOLOæ£€æµ‹ï¼ˆåªæ£€æµ‹äººï¼‰
        results = yolo.predict(
            source=frame,
            device="mps",
            conf=0.3,
            verbose=False,
            classes=[0]  # åªæ£€æµ‹äººï¼ˆclass 0ï¼‰
        )[0]
        
        # æå–æ£€æµ‹ç»“æœ
        detections = []
        if results.boxes is not None and len(results.boxes) > 0:
            boxes = results.boxes
            for i in range(len(boxes)):
                x1, y1, x2, y2 = boxes.xyxy[i].cpu().numpy()
                conf = float(boxes.conf[i])
                cls = int(boxes.cls[i])
                detections.append([x1, y1, x2, y2, conf, cls])
        
        # ByteTrackè·Ÿè¸ª
        if len(detections) > 0:
            detections = np.array(detections)
            tracks = tracker.update(detections, frame)
            
            if tracks.size > 0:
                for track in tracks:
                    x1, y1, x2, y2 = map(int, track[:4])
                    track_id = int(track[4])
                    
                    # è®°å½•è¿™ä¸ªäºº
                    if track_id not in people:
                        # ç¬¬ä¸€æ¬¡å‡ºç°ï¼Œè£å‰ªå¹¶ä¿å­˜å›¾ç‰‡
                        crop = frame[y1:y2, x1:x2]
                        if crop.size > 0:
                            crop_path = output_crops_dir / f"id{track_id:03d}.jpg"
                            cv2.imwrite(str(crop_path), crop)
                            
                            people[track_id] = {
                                "image": str(crop_path),
                                "frames": [frame_idx],
                                "first_bbox": (x1, y1, x2, y2),
                                "all_bboxes": [(frame_idx, x1, y1, x2, y2)]
                            }
                    else:
                        # å·²ç»å­˜åœ¨ï¼Œåªè®°å½•å¸§å·
                        people[track_id]["frames"].append(frame_idx)
                        people[track_id]["all_bboxes"].append((frame_idx, x1, y1, x2, y2))
    
    cap.release()
    elapsed = time.time() - start_time
    
    print(f"\n\nâœ… è§†é¢‘å¤„ç†å®Œæˆï¼")
    print(f"   ç”¨æ—¶: {elapsed:.1f}ç§’")
    print(f"   æ£€æµ‹åˆ° {len(people)} ä¸ªä¸åŒçš„äºº")
    
    # è¿‡æ»¤å‡ºç°å¤ªçŸ­çš„äººï¼ˆå¯èƒ½æ˜¯è¯¯æ£€ï¼‰
    min_frames = 5
    filtered_people = {
        tid: data for tid, data in people.items()
        if len(data["frames"]) >= min_frames
    }
    
    print(f"   è¿‡æ»¤åå‰©ä½™ {len(filtered_people)} ä¸ªæœ‰æ•ˆç›®æ ‡")
    
    return filtered_people


# ===== 2. VLMæŸ¥è¯¢æ¨¡å— =====
class VLMQueryEngine:
    """VLMæŸ¥è¯¢å¼•æ“"""
    
    def __init__(self):
        print("\nğŸ”§ åŠ è½½VLMæ¨¡å‹...")
        
        # æ£€æµ‹è®¾å¤‡
        if torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"
        
        # åŠ è½½Qwen2-VL
        model_name = "Qwen/Qwen2-VL-2B-Instruct"
        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "mps" else torch.float32,
            device_map=self.device
        )
        self.processor = AutoProcessor.from_pretrained(model_name)
        
        print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {self.device.upper()})")
    
    def ask(self, image_path, question):
        """å¯¹å›¾ç‰‡æé—®"""
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": str(image_path)},
                    {"type": "text", "text": question}
                ]
            }
        ]
        
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            generated_ids = self.model.generate(**inputs, max_new_tokens=30)
        
        generated_ids_trimmed = [
            out_ids[len(in_ids):]
            for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)
        ]
        
        answer = self.processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        return answer.strip()


# ===== 3. æœç´¢å¼•æ“ =====
def search_person_by_description(people, vlm_engine, description):
    """
    æ ¹æ®æè¿°åœ¨è§†é¢‘ä¸­æ‰¾äºº
    
    å‚æ•°:
        people: process_video_and_extract_people()è¿”å›çš„æ•°æ®
        vlm_engine: VLMæŸ¥è¯¢å¼•æ“
        description: ç”¨æˆ·æè¿°ï¼Œä¾‹å¦‚"ç©¿çº¢è‰²è¡£æœçš„äºº"
    
    è¿”å›:
        åŒ¹é…çš„track_idåˆ—è¡¨
    """
    print("\n" + "=" * 70)
    print(f"ğŸ” é˜¶æ®µ2: æ ¹æ®æè¿°æ‰¾äºº")
    print("=" * 70)
    print(f"   æŸ¥è¯¢: {description}")
    print(f"   å€™é€‰äººæ•°: {len(people)}\n")
    
    # æ„å»ºé—®é¢˜ï¼ˆæ ¹æ®æè¿°ç±»å‹ï¼‰
    if "çº¢è‰²" in description or "è“è‰²" in description or "ç»¿è‰²" in description or "é¢œè‰²" in description:
        question = "What is the main color of this person's clothing? Answer with one word only."
        target_keyword = None
        if "çº¢è‰²" in description:
            target_keyword = "red"
        elif "è“è‰²" in description:
            target_keyword = "blue"
        elif "ç»¿è‰²" in description:
            target_keyword = "green"
        elif "é»‘è‰²" in description:
            target_keyword = "black"
        elif "ç™½è‰²" in description:
            target_keyword = "white"
    
    elif "èƒŒåŒ…" in description or "backpack" in description.lower():
        question = "Is this person carrying a backpack? Answer yes or no."
        target_keyword = "yes"
    
    elif "å¸½å­" in description or "hat" in description.lower():
        question = "Is this person wearing a hat? Answer yes or no."
        target_keyword = "yes"
    
    else:
        # é€šç”¨æè¿°ï¼Œç›´æ¥ç”¨è‡ªç„¶è¯­è¨€
        question = f"Does this person match this description: '{description}'? Answer yes or no."
        target_keyword = "yes"
    
    print(f"   VLMé—®é¢˜: {question}")
    print(f"   åŒ¹é…å…³é”®è¯: {target_keyword}\n")
    
    # éå†æ‰€æœ‰äººï¼Œé€ä¸ªæé—®
    results = []
    
    for idx, (track_id, data) in enumerate(people.items(), 1):
        image_path = data["image"]
        
        print(f"[{idx}/{len(people)}] ID {track_id:3d} ... ", end='')
        
        # æé—®VLM
        answer = vlm_engine.ask(image_path, question)
        answer_lower = answer.lower().strip()
        
        # åˆ¤æ–­æ˜¯å¦åŒ¹é…
        is_match = False
        if target_keyword:
            is_match = target_keyword.lower() in answer_lower
        
        if is_match:
            results.append(track_id)
            print(f"âœ… åŒ¹é…ï¼ (å›ç­”: {answer})")
        else:
            print(f"âŒ ä¸åŒ¹é… (å›ç­”: {answer})")
    
    return results


# ===== 4. ç»“æœå¯è§†åŒ– =====
def visualize_results(video_path, people, matched_ids, output_path):
    """
    å¯è§†åŒ–æœç´¢ç»“æœ
    
    1. åœ¨è§†é¢‘ä¸­æ ‡æ³¨åŒ¹é…çš„äºº
    2. æ‹¼æ¥åŒ¹é…çš„äººç‰©å›¾ç‰‡
    """
    if not matched_ids:
        print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„äºº")
        return
    
    print(f"\nğŸ“Š å¯è§†åŒ–ç»“æœ...")
    
    # 1. æ‹¼æ¥äººç‰©å›¾ç‰‡
    print("   ç”Ÿæˆäººç‰©æ‹¼å›¾...")
    images = []
    for tid in matched_ids[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
        img = cv2.imread(people[tid]["image"])
        if img is not None:
            # è°ƒæ•´å¤§å°
            h, w = img.shape[:2]
            target_h = 200
            scale = target_h / h
            new_w = int(w * scale)
            img = cv2.resize(img, (new_w, target_h))
            
            # æ·»åŠ IDæ ‡ç­¾
            cv2.rectangle(img, (0, 0), (new_w, 40), (0, 0, 0), -1)
            cv2.putText(
                img, f"ID: {tid}",
                (10, 28),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.8, (0, 255, 0), 2
            )
            images.append(img)
    
    if images:
        collage = np.hstack(images)
        collage_path = output_path.parent / f"{output_path.stem}_collage.jpg"
        cv2.imwrite(str(collage_path), collage)
        print(f"   âœ… äººç‰©æ‹¼å›¾: {collage_path}")
    
    # 2. åœ¨è§†é¢‘ä¸­æ ‡æ³¨ï¼ˆå–ç¬¬ä¸€å¸§ï¼‰
    print("   ç”Ÿæˆæ ‡æ³¨è§†é¢‘å¸§...")
    cap = cv2.VideoCapture(str(video_path))
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„äººç¬¬ä¸€æ¬¡å‡ºç°çš„å¸§
    first_match_id = matched_ids[0]
    first_frame_idx = people[first_match_id]["frames"][0]
    
    cap.set(cv2.CAP_PROP_POS_FRAMES, first_frame_idx - 1)
    ret, frame = cap.read()
    
    if ret:
        # æ ‡æ³¨æ‰€æœ‰åŒ¹é…çš„äººï¼ˆå¦‚æœåœ¨è¿™ä¸€å¸§å‡ºç°ï¼‰
        for tid in matched_ids:
            if first_frame_idx in people[tid]["frames"]:
                # æ‰¾åˆ°è¿™ä¸€å¸§çš„bbox
                for frame_num, x1, y1, x2, y2 in people[tid]["all_bboxes"]:
                    if frame_num == first_frame_idx:
                        # ç»˜åˆ¶ç»¿è‰²æ¡†
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)
                        cv2.putText(
                            frame, f"ID:{tid}",
                            (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            0.8, (0, 255, 0), 2
                        )
        
        frame_path = output_path
        cv2.imwrite(str(frame_path), frame)
        print(f"   âœ… æ ‡æ³¨å¸§: {frame_path}")
    
    cap.release()


# ===== 5. ä¸»å‡½æ•° =====
def main():
    """
    ä¸»æµç¨‹ï¼šæ ¹æ®æè¿°åœ¨è§†é¢‘ä¸­æ‰¾äºº
    """
    print("\n" + "=" * 70)
    print("ğŸ¯ è§†é¢‘ä¸­æ‰¾äººç³»ç»Ÿ")
    print("=" * 70)
    
    # é˜¶æ®µ1: å¤„ç†è§†é¢‘ï¼Œæå–æ‰€æœ‰äºº
    crops_dir = OUTPUT_DIR / "crops"
    people = process_video_and_extract_people(VIDEO_PATH, crops_dir)
    
    # ä¿å­˜äººç‰©æ•°æ®
    people_db_path = OUTPUT_DIR / "people_database.json"
    with open(people_db_path, 'w', encoding='utf-8') as f:
        json.dump({
            str(k): {
                "image": v["image"],
                "num_frames": len(v["frames"]),
                "first_frame": v["frames"][0],
                "first_bbox": v["first_bbox"]
            }
            for k, v in people.items()
        }, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ äººç‰©æ•°æ®å·²ä¿å­˜: {people_db_path}")
    
    # é˜¶æ®µ2: åŠ è½½VLM
    vlm_engine = VLMQueryEngine()
    
    # é˜¶æ®µ3: æ‰§è¡Œæœç´¢
    test_queries = [
        "ç©¿çº¢è‰²è¡£æœçš„äºº",
        "ç©¿è“è‰²è¡£æœçš„äºº",
        "èƒŒèƒŒåŒ…çš„äºº",
    ]
    
    for query in test_queries:
        matched_ids = search_person_by_description(people, vlm_engine, query)
        
        print("\n" + "=" * 70)
        print(f"ğŸ“‹ æŸ¥è¯¢ç»“æœ: {query}")
        print("=" * 70)
        
        if matched_ids:
            print(f"âœ… æ‰¾åˆ° {len(matched_ids)} ä¸ªåŒ¹é…çš„äºº:")
            for tid in matched_ids:
                print(f"   - Track ID: {tid}")
                print(f"     å‡ºç°å¸§æ•°: {len(people[tid]['frames'])}")
                print(f"     é¦–æ¬¡å‡ºç°: ç¬¬ {people[tid]['frames'][0]} å¸§")
            
            # å¯è§†åŒ–
            output_path = OUTPUT_DIR / f"result_{query}.jpg"
            visualize_results(VIDEO_PATH, people, matched_ids, output_path)
        else:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„äºº")
        
        print()
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 70)
    print("âœ… æœç´¢å®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"   - crops/              äººç‰©è£å‰ªå›¾ç‰‡")
    print(f"   - people_database.json  äººç‰©æ•°æ®åº“")
    print(f"   - result_*.jpg         æœç´¢ç»“æœå¯è§†åŒ–")


if __name__ == '__main__':
    main()

