# ğŸ—ï¸ Edge-Detective ç³»ç»Ÿæ¶æ„å‡çº§ä¸é‡æ„è®¾è®¡ä¹¦ (Phase 2)

> **ç‰ˆæœ¬**: 2.0 (Draft)
> **å¯¹é½ç›®æ ‡**: [Industrialization Roadmap - Phase 2](./industrialization_roadmap.md)
> **æ ¸å¿ƒç†å¿µ**: è§£è€¦ (Decoupling)ã€å¼‚æ­¥ (Async)ã€æœåŠ¡åŒ– (Service-Oriented)

---

## 1. ç°çŠ¶å®¡è®¡ä¸"è…æœ½ä»£ç "æ¸…æ´—è®¡åˆ’ (The Purge List)

ä¸ºäº†è¾¾æˆå·¥ä¸šåŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬éœ€è¦ç—›ä¸‹å†³å¿ƒï¼Œæ¸…ç†ä¸ç¬¦åˆäº‘åŸç”Ÿæ¶æ„çš„ä»£ç ã€‚

### ğŸš¨ å¾…åˆ é™¤/åºŸå¼ƒç»„ä»¶ (Deprecated Components)

| ç»„ä»¶/æ–‡ä»¶ | åˆ¤å®š | ç†ç”± (Why it's bad) | æ›¿ä»£æ–¹æ¡ˆ |
| :--- | :--- | :--- | :--- |
| **`src/pipeline/vlm_client_hf.py`** | **ğŸ”¥ DELETE** | **åæ¨¡å¼æ ¸å¿ƒ**ã€‚ç›´æ¥åœ¨ä¸šåŠ¡è¿›ç¨‹ä¸­åŠ è½½ 16GB æ¨¡å‹æƒé‡ã€‚å¯¼è‡´æ— æ³•æ°´å¹³æ‰©å±• API æœåŠ¡ï¼Œä¸”å—é™äº Python GIL å’Œ PyTorch è°ƒåº¦ç“¶é¢ˆï¼Œå¼•å‘ Padding æ€§èƒ½é—®é¢˜ã€‚ | **vLLM Service** (ç‹¬ç«‹è¿›ç¨‹) + **HTTP Client** |
| `src/core/config.py` (éƒ¨åˆ†å­—æ®µ) | **REFACTOR** | æ··æ‚äº†"åŸºç¡€è®¾æ–½é…ç½®"ï¼ˆæ˜¾å¡å‹å·ï¼‰ä¸"ä¸šåŠ¡é…ç½®"ã€‚`yolo_device`, `vlm_batch_size` ç­‰åº•å±‚ç¡¬ä»¶å‚æ•°ä¸åº”ç”±ä¸šåŠ¡ä»£ç ç®¡ç†ã€‚ | é…ç½®åº”æ‹†åˆ†ä¸º `AppConfig` (ä¸šåŠ¡) å’Œ `InfraConfig` (K8s/Env)ã€‚ç¡¬ä»¶å‚æ•°ç§»äº¤ç»™ `docker-compose.yaml` æˆ–å¯åŠ¨è„šæœ¬ã€‚ |
| `src/pipeline/recall.py` (åŒæ­¥é€»è¾‘) | **REWRITE** | æ ¸å¿ƒè°ƒåº¦é€»è¾‘æ˜¯åŒæ­¥é˜»å¡çš„ï¼ˆSerial Blockingï¼‰ã€‚åœ¨è°ƒç”¨ VLM æ—¶æ•´ä¸ªçº¿ç¨‹å¡æ­»ï¼Œæ— æ³•å¤„ç†å¹¶å‘è¯·æ±‚ã€‚ | **Async/Await** é‡å†™ã€‚ä½¿ç”¨ `asyncio.gather` å¹¶å‘è¯·æ±‚æ¨ç†æœåŠ¡ã€‚ |
| æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¾èµ– (File I/O) | **PHASE OUT** | ä»£ç ä¸­å¤§é‡å‡ºç°çš„ `open(path)`ã€‚å¦‚æœåœ¨ K8s å¤šèŠ‚ç‚¹éƒ¨ç½²ï¼ŒWorker A æ— æ³•è¯»å– Worker B å­˜çš„å›¾ç‰‡ã€‚ | **Object Storage (MinIO)** æŠ½è±¡å±‚ã€‚ |

---

## 2. ç›®æ ‡æ¶æ„è®¾è®¡ (Target Architecture)

æˆ‘ä»¬å°†ä» **Monolithic Script (å•ä½“è„šæœ¬)** è½¬å‹ä¸º **Model-as-a-Service (æ¨¡å‹å³æœåŠ¡)** æ¶æ„ã€‚

### 2.1 ç³»ç»Ÿæ‹“æ‰‘å›¾

```mermaid
graph TD
    subgraph "Control Plane (CPU-Bound)"
        API[FastAPI Gateway] -->|Async HTTP| Orchestrator[Pipeline Orchestrator]
        Orchestrator -->|Read/Write| DB[(PostgreSQL)]
        Orchestrator -->|Read/Write| VectorDB[(Qdrant)]
        Orchestrator -->|Put/Get| ObjStore[(MinIO)]
    end

    subgraph "Inference Plane (GPU-Bound)"
        Orchestrator -- "OpenAI API Protocol" --> vLLM[vLLM Server (Qwen2-VL)]
        Orchestrator -- "gRPC/HTTP" --> Detection[YOLO/SigLIP Service]
    end

    subgraph "Infrastructure"
        vLLM -->|Mapped| GPU[L4 GPU]
    end
```

### 2.2 å…³é”®æ¶æ„å†³ç­–

1.  **æ¨ç†ä¸ä¸šåŠ¡ç‰©ç†éš”ç¦»**ï¼š
    *   **æ¨ç†å±‚ (Inference Layer)**ï¼švLLM ç‹¬å  GPUã€‚å®ƒåªè´Ÿè´£è®¡ç®—ï¼Œä¸çŸ¥é“ä»€ä¹ˆæ˜¯ "Track" æˆ– "Evidence"ï¼ŒåªçŸ¥é“ "Input Tokens -> Output Tokens"ã€‚
    *   **ä¸šåŠ¡å±‚ (Business Layer)**ï¼šPython ä¸šåŠ¡ä»£ç åªè´Ÿè´£é€»è¾‘åˆ¤æ–­ã€‚å®ƒä¸çŸ¥é“æ¨¡å‹æ˜¯è·‘åœ¨æœ¬åœ°è¿˜æ˜¯è·‘åœ¨ç«æ˜Ÿä¸Šã€‚

2.  **é€šä¿¡åè®®æ ‡å‡†åŒ–**ï¼š
    *   æ‰€æœ‰ VLM äº¤äº’å¼ºåˆ¶ä½¿ç”¨ **OpenAI API å…¼å®¹åè®®**ã€‚
    *   **ä¼˜åŠ¿**ï¼šå¦‚æœæˆ‘ä»¬æ˜å¤©æƒ³æµ‹è¯• GPT-4o æˆ– Claude 3.5 Sonnetï¼Œåªéœ€æ”¹ä¸€ä¸ª URL é…ç½®ï¼Œä»£ç ä¸€è¡Œä¸ç”¨åŠ¨ã€‚

3.  **IO ä¹Ÿæ˜¯å¹¶è¡Œçš„**ï¼š
    *   åœ¨æ—§æ¶æ„ä¸­ï¼ŒVLM åˆ†æ Track A æ—¶ï¼ŒCPU æ˜¯é—²ç½®çš„ã€‚
    *   åœ¨æ–°æ¶æ„ä¸­ï¼ŒOrchestrator å¯ä»¥åŒæ—¶å‘ vLLM å‘é€ 10 ä¸ª Track çš„è¯·æ±‚ï¼ˆvLLM å†…éƒ¨ä¼šè‡ªåŠ¨åš Continuous Batchingï¼‰ï¼ŒåŒæ—¶å‘æ•°æ®åº“å†™å…¥å…ƒæ•°æ®ã€‚

---

## 3. é‡æ„å®æ–½æ–¹æ¡ˆ (Implementation Plan)

### Step 1: éƒ¨ç½²æ¨ç†æœåŠ¡ (Infrastructure)

æˆ‘ä»¬éœ€è¦å…ˆè®©æ¨¡å‹ä½œä¸ºç‹¬ç«‹æœåŠ¡è·‘èµ·æ¥ã€‚è¿™æ˜¯æ‰€æœ‰é‡æ„çš„å‰æã€‚

**æ–°å»ºæ–‡ä»¶: `deploy/start_vllm.sh`**
```bash
# å·¥ä¸šçº§å¯åŠ¨å‚æ•°
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2-VL-7B-Instruct \
    --trust-remote-code \
    --host 0.0.0.0 \
    --port 8000 \
    --gpu-memory-utilization 0.90 \
    --max-model-len 8192 \
    --limit-mm-per-prompt image=5 \
    --enable-prefix-caching  # å¼€å¯å‰ç¼€ç¼“å­˜ï¼Œå¤§å¹…åŠ é€Ÿç›¸åŒ Prompt çš„æ£€ç´¢
```

### Step 2: ç¼–å†™æ–°çš„ VLM å®¢æˆ·ç«¯ (Code)

**æ–°å»ºæ–‡ä»¶: `src/pipeline/vlm_client_vllm.py`**
è¯¥å®¢æˆ·ç«¯å®Œå…¨**æ— çŠ¶æ€**ï¼Œä»…å°è£… HTTP è°ƒç”¨ã€‚

```python
import base64
from openai import AsyncOpenAI
from core.config import SystemConfig

class VLMClientVLLM:
    def __init__(self, config: SystemConfig):
        # ä½¿ç”¨æ ‡å‡† OpenAI å®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            base_url=config.vlm_api_url,  # e.g., "http://localhost:8000/v1"
            api_key="EMPTY"
        )
        self.model = config.vlm_model_name

    def _encode_image(self, image_path: str) -> str:
        """å·¥ä¸šåŒ–å¤„ç†ï¼šæœªæ¥è¿™é‡Œå¯æ›¿æ¢ä¸ºç›´æ¥ç”Ÿæˆ MinIO Presigned URL"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    async def answer_track(self, track: EvidencePackage, question: str) -> QueryResult:
        """å¼‚æ­¥å•æ¡å¤„ç† - å¹¶å‘ç”±è°ƒç”¨æ–¹æ§åˆ¶"""
        # æ„é€  OpenAI æ ¼å¼æ¶ˆæ¯...
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.1
        )
        return self._parse(response)
```

### Step 3: é‡æ„æ ¸å¿ƒæµæ°´çº¿ (Async Orchestration)

**ä¿®æ”¹: `src/pipeline/recall.py`**

å°†åŸæœ¬çš„ `for` å¾ªç¯ä¸²è¡Œè°ƒç”¨æ”¹ä¸º `asyncio.gather`ã€‚

```python
# æ—§ä»£ç  (The Bad)
# for package in candidates:
#     result = client.answer(package)  <-- é˜»å¡

# æ–°ä»£ç  (The Good)
import asyncio

class AsyncRecallEngine:
    async def process_candidates(self, candidates, question):
        tasks = []
        for package in candidates:
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            tasks.append(self.vlm_client.answer_track(package, question))
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
        # vLLM æœåŠ¡ç«¯ä¼šè‡ªåŠ¨å¤„ç†è¿™äº›å¹¶å‘è¯·æ±‚çš„ Batching (Continuous Batching)
        results = await asyncio.gather(*tasks)
        return results
```

---

## 4. æ”¶ç›Šé¢„æµ‹ (ROI)

é€šè¿‡è¿™æ¬¡æ¶æ„å‡çº§ï¼Œæˆ‘ä»¬å°†è·å¾—ï¼š

1.  **é€Ÿåº¦è´¨å˜**ï¼š
    *   ä¸å†å—é™äº Paddingã€‚vLLM çš„ Continuous Batching èƒ½è®©ååé‡æå‡ **3-5 å€**ã€‚
    *   Python ç«¯ä¸å†é˜»å¡ï¼Œå¯ä»¥å¤„ç†å…¶ä»– I/Oã€‚
2.  **è°ƒè¯•å‹å¥½**ï¼š
    *   æ¨¡å‹æœåŠ¡ä¸€ç›´å¼€ç€ï¼Œè°ƒè¯• Python ä»£ç æ—¶ä¸éœ€è¦æ¯æ¬¡éƒ½ç­‰ 2 åˆ†é’ŸåŠ è½½æ¨¡å‹ã€‚**ä¿®æ”¹ä»£ç  -> è¿è¡Œ** çš„åé¦ˆå¾ªç¯ç¼©çŸ­åˆ° 1 ç§’ã€‚
3.  **æœªæ¥å°±ç»ª**ï¼š
    *   ç›´æ¥å¯¹æ¥ Phase 3 çš„ K8s éƒ¨ç½²ã€‚FastAPI å®¹å™¨å’Œ vLLM å®¹å™¨å¯ä»¥åˆ†åˆ«æ‰©å®¹ã€‚

---

## 5. ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®

1.  **æ‰¹å‡†åˆ é™¤**: ç¡®è®¤åºŸå¼ƒ `vlm_client_hf.py`ã€‚
2.  **ç¯å¢ƒå‡†å¤‡**: åœ¨æ‚¨çš„ L4 æœºå™¨ä¸Šå¯åŠ¨ vLLM Serverã€‚
3.  **ä»£ç æ›¿æ¢**: æˆ‘å°†ä¸ºæ‚¨ç¼–å†™ä¸Šè¿°çš„ `vlm_client_vllm.py` å’Œå¼‚æ­¥ç‰ˆçš„ `recall.py`ã€‚

```