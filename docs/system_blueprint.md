# 🎯 视频语义检索与行为分析系统蓝图

本文档描述本项目的长期目标和阶段规划，用来对齐我们“现在在做什么、下一步要做什么、哪些只在远期考虑”。

---

## 1. 总体目标

构建一个 **多目标视频理解系统**，能够在长时间、多人物/多车辆的视频中：

- 完成可靠的 **检测 + 跟踪 + 轨迹统计**；
- 支持用户用自然语言提出开放问题，例如：
  - 「帮我找出穿紫色衣服的人」
  - 「有没有人在可疑地尾随别人？」
  - 「这辆车大概时速是多少？」
- 给出 **可视化证据**（高亮轨迹、关键帧截图、时间区间、统计报表），而不是只给一段文字回答。

系统分为三个主要层次：

1. 感知层（Perception）：YOLO / 跟踪 / 轨迹与裁剪图生成
2. 语义层（Semantic）：把轨迹压缩成“证据包”，结合 VLM/LLM 做判断
3. 检索与问答层（Retrieval & QA）：把用户问题翻译成对证据包的查询，返回结果与解释

---

## 2. 分阶段目标（基于当前思路）

> 目前代码大致处在 **Phase 0.5**：感知 + 简单运动特征已经有了，真正“问题驱动的人检索”还没开始做。

### 2.1 Phase 0 —— 轨迹索引与可视化 demo（当前状态）

已完成能力（`src/pipeline/` + `src/video_semantic_search.py`）：

- **感知层**
  - 使用 YOLOv11 + ByteTrack 对单段离线视频做 **行人检测与多目标跟踪**；
  - 为每个 `track_id` 收集：
    - 出现的帧号 `frames`
    - 每帧的边界框 `bboxes`
    - 若干关键帧裁剪 `crops`（采样保存到磁盘）
  - 能在原视频上对指定 `track_id` 进行 **高亮渲染**，输出 `tracking_xxx.mp4`。

- **简单运动特征**
  - 为每个轨迹计算：
    - 平均速度 `avg_speed_px_s`
    - 最大速度 `max_speed_px_s`
    - 轨迹长度 `path_length_px`
    - 持续时间 `duration_s`
  - 这些特征目前主要用于调试和未来行为分析的基础。

- **现有语义 / 检索代码**
  - 目前的 `color / has_backpack` 等字段只是实验原型，证明 VLM 可以读裁剪图；
  - 真正的目标是 **“问题驱动 QA”**，这些固定字段会在 Phase 1 中被弱化甚至废弃。

> 本阶段的目的：把视频可靠地切成“按人/track 的证据包”，并确认检测/跟踪/裁剪/高亮视频等基础链路是稳定的。

### 2.2 Phase 1 —— 单视频、人检索、问题驱动 QA

短期核心目标：**只考虑“找人”这一类问题（衣服颜色、帽子形状、是否携带物品等），不预先大量设计规则字段，让用户可以用一句自然语言描述在一条视频里找到人。**

#### 2.2.1 索引阶段（离线，只做一次）

- 使用 Phase 0 的感知层：
  - 视频 → YOLOv11 + ByteTrack → `track_id → {frames, bboxes, crops}`；
  - 保留简单运动特征（速度/停留时间等），为未来问题做准备。
- 为每个 `track` 构建一个 **证据包（evidence package）**：
  - 若干代表性裁剪图（同一人的多张图）；
  - 简单数值特征（速度、持续时间等）。

#### 2.2.2 查询阶段（问题驱动）

- 输入：
  - 用户的自然语言问题，例如：
    - 「找出穿紫色衣服的人」
    - 「找出戴牛仔帽的人」
    - 「找出背圆形背包的人」

- 流程：
  1. 在需要时，用 CLIP 或颜色直方图做**宽松的粗筛**，选出一批候选 `track_id`（只减负，不裁决；规模小的时候可以跳过这一步，全量送给模型）。
  2. 将所有候选轨迹的证据包与问题一起喂给 Qwen2‑VL，可以一次性或分批构造统一的提示：  
     - 让模型在“看到所有候选人”的前提下回答：哪些人符合描述、各自在第几秒到第几秒出现、以及简短理由。  
     - 理想的回答形式类似于一个列表：`[{track_id, start_s, end_s, score, reason}, ...]`。
  3. 系统解析模型返回的列表，按置信度 `score` 排序，并据此生成最终结果：
     - 对每个命中的 `track_id`，记录其起止时间（由帧号 → 秒数对齐原视频时间轴）；
     - 保存一句解释（例如：“wearing a purple jacket and a black backpack”）；
     - 基于这些轨迹在原视频上画红框，高亮导出 `tracking_描述.mp4` 和拼图截图。

- Phase 1 完成判定：
  - 对一条视频和任意一条“描述某类人的自然语言问题”，系统能**稳定**给出：
    - 一批匹配轨迹；
    - 每个轨迹在原视频中的清晰时间区间（以秒为单位）；
    - 可视化证据（高亮视频 + 截图）；
  - 整个过程不依赖提前写死大量属性字段，主要靠“问题 + 证据包 → VLM 返回结构化结果”，我们只负责把结果重新映射回时间轴和视频。

---

## 3. 中期规划：从“找人”扩展到行为与事件

在 Phase 1 稳定后，继续沿用「感知 → 证据包 → 问题驱动 QA」的思想，逐步增强能力：

### 3.1 丰富运动与交互特征

- 在 `TrackFeatureExtractor` 基础上增加：
  - 不同空间区域内的停留时间（dwell time），例如门口、收银台等 ROI；
  - 两个 track 之间的长时间接近/同行关系（potential following / meeting）；
  - 突然加速、急停等异常运动模式。

这些特征将支持问题：

- 「有没有人在门口徘徊超过 30 秒？」
- 「有没有人长时间跟着另一个人？」

### 3.2 事件级语义建模

- 在 track 之上引入 **事件对象（Event）**：
  - 示例：`(person_A, person_B, time_range, interaction_type="following")`；
  - 示例：`(vehicle_X, time_range, event_type="speeding", speed_estimate=...)`。
- 利用简单规则 + VLM/LLM 帮忙命名事件：
  - 先用规则/特征筛出候选片段；
  - 把短视频/关键帧 + 特征摘要交给模型，生成事件描述：
    - “Person A appears to be closely following Person B for about 40 seconds.”

### 3.3 更强的文本检索与问答

- 将轨迹/事件的描述编码为向量：
  - 使用文本嵌入模型或 VLM 的文本头；
  - 支持「语义相似度」检索而不是仅仅字符串匹配。
- 问答方式：
  - 用户问题 → 向量化 → 检索最相关的若干 track / 事件 →
  - 将这些候选的描述 + 统计 + 截图打包给 LLM，让它生成最终回答与证据列表。

示例问题：

- 「有没有看起来在进行可疑交易的人？」
- 「请列出跑得最快的三个人，并估算他们的速度。」

---

## 4. 远期规划（future 中的 future）：多视频、并发与系统化部署

> 本节只是远期愿景，短期不会强行实现。重点是先把 **Phase 1** 做扎实。

### 4.1 多流与实时处理

- 支持多路摄像头/RTSP 流：
  - 使用队列（Kafka/Redis Stream 等）收集检测与轨迹；
  - 后端 worker 异步完成索引与事件检测。
- 提供 Web/API 接口：
  - 用户通过前端输入自然语言指令（例如「请监控所有红衣人」）；
  - 系统持续推送告警和可视化结果。

### 4.2 向量数据库与缓存（包括 Redis 等）

- 将轨迹/事件向量存入：
  - 向量数据库（如 Milvus / Qdrant / PGVector）做大规模检索；
  - Redis 用于两类用途：
    - 缓存索引和 QA 结果；
    - 作为任务队列支撑多 worker 并发处理索引/问答任务。

### 4.3 统一监控与评估

- 指标体系：
  - 检测/跟踪精度：MOTA、IDF1 等；
  - 检索质量：Top-k Recall、人工打分；
  - 系统性能：延迟、吞吐、GPU/内存占用。
- 提供可视化 Dashboard，用于观测整套系统行为。

---

## 5. 设计原则回顾

1. **分层清晰**：感知、特征、语义、检索解耦，底层模型可替换（如 YOLOv8 → YOLOv11）。
2. **问题驱动优先**：从“按对象构建证据包 + 让模型在问题下判断”出发，而不是预先写死所有语义字段。
3. **先离线、再实时**：先在单视频离线场景打磨 Phase 1，再考虑多视频、Redis、Kubernetes 等工程化扩展。
4. **可解释性优先**：不仅返回答案，还要能给出轨迹、关键帧、时间区间和数值特征作为证据。

本蓝图会随着实现进展不断更新，用于对齐工程实现与研究探索的方向。
