# 🎯 系统蓝图（简版）——视频里“用人话找人”的系统

> 这是一份给“我们自己看得懂”的系统规划：现在做到哪、下一步做什么、以后大概往哪长。

---

## 1. 想做成什么样？

我们要做的是一个可以在**长视频、多行人**场景下工作的系统：

- 能稳定完成：检测 + 跟踪 + 轨迹统计；
- 用户可以用一句自然语言提问：  
  「帮我找出穿紫色衣服的人」「找戴牛仔帽的人」「找背圆形包的人」；
- 系统要返回：
  - 找到了哪些人（track_id）；
  - 每个人大概出现在视频的哪一段时间（起止秒）；
  - 带红框的视频和拼在一起的截图；
  - 一句简单解释（为什么认为这是目标）。

整个系统围绕一个核心抽象：**轨迹 Track**。  
一条 Track = 同一个人在视频中的一段连续运动 + 若干代表性图片 + 一些数值特征。

---

## 2. 阶段划分

> 当前代码大概在 **Phase 0.5**：感知和轨迹特征已经有了，但“问题驱动找人”的主流程还没真正做完。

### 2.1 Phase 0 —— 只负责“切干净”（已基本完成）

目标：把视频可靠地切成“按人/track 的证据包”，并能在视频上高亮轨迹。

- **感知层**
  - YOLOv11 + ByteTrack，对单条视频做多目标跟踪；
  - 对每个 `track_id` 记录：帧号 `frames`、框 `bboxes`、裁剪图 `crops`；
  - 能按给定的 track_id 列表在原视频上画红框，导出 `tracking_xxx.mp4`。

- **运动特征**
  - 对每条轨迹计算：平均速度、最大速度、路径长度、持续时间；
  - 用于：
    - 检查跟踪质量；
    - 为以后“徘徊、尾随、超速”等行为问题预留结构化信息。

- **现有语义代码**
  - 之前的 `color / has_backpack` 等字段只是实验品，只证明“VLM 能看裁剪图”；
  - 真正方案是 Phase 1 的“问题驱动 QA”，不会继续扩这套固定字段。

---

### 2.2 Phase 1 —— 单视频、人检索、问题驱动 QA（现在要做的）

**一句话版本：**  
先把每个人切成“证据包”，用户问一句话，系统把所有人（或候选人）的证据包交给 VLM，让它选出谁符合描述、大概在哪几秒出现，然后系统帮忙画框和导出视频。

#### 2.2.1 索引阶段（离线一次）

输入：一条视频。  
输出：`track_id → EvidencePackage`。

- 使用 Phase 0 的感知层：
  - 视频 → YOLOv11 + ByteTrack → `track_id → {frames, bboxes, crops}`；
  - 同时计算简单运动特征（速度 / 时长等）。
- 为每个 track 构建 **EvidencePackage（证据包）**，主要包含：
  - 若干代表性裁剪图（同一人多帧采样，数量控制在 3–6 张）；
  - 帧号、FPS，用来对齐时间轴；
  - 简单数值特征（平均速度、持续时间等）；
  - 预留扩展字段（以后用于行为标签、场景信息）。

证据包是后续所有检索 / QA 的唯一输入形式，上游模型怎么换都不影响这一层。

#### 2.2.2 查询阶段（两步：召回 + VLM 精排）

输入：一句话问题，例如：

- 「找出穿紫色衣服的人」
- 「找出戴牛仔帽的人」
- 「找出背圆形背包的人」

输出：一批匹配的轨迹 + 时间区间 + 理由 + 高亮视频。

**Step 1：轻量召回（RecallEngine）**

- 目标：从所有轨迹中选出一批候选（比如 20～50 条），降低 VLM 的工作量；
- 手段（可以逐步增强）：
  - v0：直接返回所有轨迹（即“无召回”的 degenerate 版本）；
  - v1：用 CLIP / 颜色直方图/ 简单规则做宽松过滤，保证高召回率。
- 召回阶段只负责“减负”，不做最终决策，错杀风险需要在实验里评估。

**Step 2：VLM 精排与判定（VLMClient）**

- 对每个候选轨迹：
  - 取该轨迹的证据包（多张裁剪 + 运动特征）；
  - 和问题一起喂给 Qwen2‑VL；
  - 让模型回答：
    - 是否匹配（yes/no 或 score）；
    - 一句简短解释（reason，如 “wearing a purple jacket and a black backpack”）。
- 系统收集所有候选的回答，过滤出匹配轨迹，按 score 排序。

内部理想的结构化输出形式类似：

```json
[
  {
    "track_id": 3,
    "start_s": 12.3,
    "end_s": 27.8,
    "score": 0.94,
    "reason": "The person wears a purple jacket and a black backpack."
  },
  ...
]
```

其中 `start_s / end_s` 由系统根据 `frames + fps` 计算，模型只负责“是不是 + 为什么”，不自己算时间轴。

**Step 3：映射回视频 & 可视化**

- 对每个命中的 `track_id`：
  - 用 `frames + fps` 得到时间区间（起止秒）；
  - 在原视频上画红框，导出 `tracking_<query>.mp4`；
  - 拼接几张代表裁剪图作为结果截图；
  - 把 `reason` 一起输出给用户。

**Phase 1 完成标准：**

- 对任意一条视频 + 任意一句“描述某类人”的自然语言：
  - 系统能稳定返回一批 track_id；
  - 每条轨迹有清晰的时间区间（秒）；
  - 有高亮视频 + 截图 + 一行解释；
  - 整条链路只依赖：EvidencePackage + RecallEngine + VLMClient 抽象，而不是提前写死一堆字段。

---

## 3. Phase 2 以后：行为与事件（只做大纲）

等 Phase 1 稳定后，再考虑这些增强功能：

### 3.1 行为特征

- 在现有运动特征基础上，增加：
  - 区域停留时间（门口、收银台等 ROI）；
  - 两人距离曲线（用于尾随 / 同行）；
  - 加速 / 急停等模式。
- 这些特征用于**发现候选片段**（例如“徘徊超过 30 秒”），不是直接由 LLM 判定。

### 3.2 事件建模

- 在轨迹之上引入 `Event` 抽象，例如：跟随、超速、聚集；
- 发现流程：规则 + 特征筛出候选 → 把短片段 + 摘要交给 VLM/LLM → 让模型写出自然语言描述。

### 3.3 语义检索

- 对轨迹 / 事件的描述做文本 embedding；
- 支持“在整个视频库中查找某种行为/事件”的语义检索；
- 查询：文本 → 向量 → 检索候选 → 再交给 LLM 精排与解释。

---

## 4. 远期（multi-video + Redis + vLLM + K8s）

> 只记目标，不在当前阶段实现。

- 多视频、多用户、多 worker 的架构：
  - Redis / Kafka 做任务队列和缓存；
  - 向量数据库存 embedding；
  - vLLM 或其他推理服务托管 VLM/LLM；
  - Kubernetes 管理各个服务的扩缩容。
- 这些只是在 **RecallEngine / VLMClient / 存储层** 换实现，对 EvidencePackage 和 Phase 1 主流程不做破坏性修改。

---

## 5. 设计原则（给自己看的 check-list）

1. **围绕轨迹抽象**
   所有上游感知结果都折叠成 `track_id → EvidencePackage`；下游逻辑只依赖它。

2. **问题驱动，而不是字段驱动**
   不预先列出一大堆属性字段，而是让用户的问题 + 证据包驱动 VLM 的判断。

3. **两阶段检索固定下来**
   轻量召回（RecallEngine） + VLM 精排（VLMClient）是固定结构，小规模时召回可以退化为“全量”，但接口不变。

4. **数值特征做“几何真相”，不是替代 VLM**
   速度、停留时间、距离等由我们计算，保证可解释和可复现；VLM 负责在这些信息和图像基础上“讲人话”。

5. **先把单视频离线场景做好**
   在一个视频上把 Phase 1 全链路打磨扎实，再考虑 Redis、向量库、K8s 等工程化扩展。

