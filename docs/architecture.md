# Edge-Detective Phase 2.6：混合双层漏斗架构
## Hybrid Two-Stage Funnel Architecture

> **文档性质**：系统架构设计白皮书  
> **核心理念**：让非视频 VLM 具备"在视频中找人"的能力  
> **本文不谈代码，只谈设计哲学、系统流转和决策逻辑**

---

## 一、核心设计哲学：倒三角漏斗 (The Funnel)

本系统的核心理念是 **"分级过滤，降本增效"**。

我们面对的挑战是：
- 📹 海量的视频数据（每秒 30 帧，每帧可能有多个人）
- 💰 昂贵的 VLM 算力（每次调用都消耗大量 Token）
- ⏱️ 实时性要求（用户不愿等待几分钟的结果）

为了在 **性能** 与 **成本** 之间找到最佳平衡点，我们将检索过程设计为一个 **三层漏斗**：

```
                    ┌─────────────────────────────────┐
                    │                                 │
     第一层         │     🔍 宽口径捕获（感知层）      │     ← 不放过任何一个人
     Perception     │     YOLO + ByteTrack            │
                    │     100% 轨迹 → 全量档案        │
                    │                                 │
                    └────────────────┬────────────────┘
                                     │
                                     ▼
                         ┌───────────────────────┐
                         │                       │
     第二层              │  ⚡ 低成本排除（召回层） │     ← 极速剔除 90% 不相关的人
     Recall              │  CLIP/SigLIP 快速安检  │
                         │  100% → 10% 候选      │
                         │                       │
                         └───────────┬───────────┘
                                     │
                                     ▼
                              ┌─────────────┐
                              │             │
     第三层                   │ 🎯 高精度审讯 │     ← 动用大模型智力深度查证
     Verification             │ 双层 VLM 验证 │
                              │ 10% → 精确匹配│
                              │             │
                              └─────────────┘
```

**漏斗哲学的核心**：

> 让便宜的模型处理海量数据，让昂贵的模型只处理最有价值的候选。
> 
> 每一层的职责都极其明确：上层负责"不漏"，下层负责"精准"。

---

## 二、证据工场：为每个人建立"数字档案"

### The Evidence Factory

在任何模型介入之前，我们首先要将 **非结构化的视频流** 转化为 **结构化的嫌疑人档案库**。

这是整个系统的基石——没有高质量的档案，后续所有的智能分析都是空中楼阁。

### 2.1 全量抓拍：一个都不能漏

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│    📹 视频流                                                             │
│     │                                                                    │
│     ▼                                                                    │
│    ┌──────────────┐         ┌──────────────┐         ┌──────────────┐   │
│    │   YOLO11n    │ ──────→ │  ByteTrack   │ ──────→ │  TrackRecord │   │
│    │   目标检测    │  检测框  │   多目标跟踪  │   轨迹   │   数字档案    │   │
│    └──────────────┘         └──────────────┘         └──────────────┘   │
│                                                                          │
│    职责：                    职责：                    产出：            │
│    在每一帧中找出            把同一个人跨帧的          每个人的完整       │
│    所有的人                  检测框关联起来            生命周期记录       │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

**设计原则**：

- **宁可误检，不可漏检**：感知层的目标是 100% 召回率，宁可把柱子当成人，也不能把人当成柱子。
- **完整生命周期**：记录每个 ID 从入画到出画的每一帧位置，这是后续一切分析的数据基础。

### 2.2 证件照选取：挑选最具代表性的瞬间

> 💡 **核心洞察**：一个人可能在视频中出现 900 帧，但我们只需要 1-2 张最好的照片就能识别他。

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│    900 帧轨迹数据                                                        │
│     │                                                                    │
│     ▼                                                                    │
│    ┌────────────────────────────────────────────────────────────────┐   │
│    │                   Best Crop Selection                          │   │
│    │                   最佳裁剪图选择算法                             │   │
│    ├────────────────────────────────────────────────────────────────┤   │
│    │                                                                │   │
│    │   评分公式：Score = 面积 × (1 + 0.5 × 中心性)                   │   │
│    │                                                                │   │
│    │   ┌─────────────────────────────────────────────────────────┐  │   │
│    │   │  面积优先                                               │  │   │
│    │   │  ├─ 离镜头越近 → 检测框越大 → 细节越丰富                 │  │   │
│    │   │  └─ 大框 = 高像素 = 能看清衣服Logo、眼镜、发型           │  │   │
│    │   │                                                         │  │   │
│    │   │  中心性加分                                             │  │   │
│    │   │  ├─ 画面中心的人 → 姿态更完整，不会被边缘裁切            │  │   │
│    │   │  └─ 边缘的人可能只露出半个身子                          │  │   │
│    │   │                                                         │  │   │
│    │   │  边缘剔除                                               │  │   │
│    │   │  └─ 贴边的框直接跳过 → 避免截断的人物                   │  │   │
│    │   └─────────────────────────────────────────────────────────┘  │   │
│    │                                                                │   │
│    └────────────────────────────────────────────────────────────────┘   │
│     │                                                                    │
│     ▼                                                                    │
│    📸 最佳"证件照"（高分辨率特写）                                       │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

**为什么不用置信度？**

传统做法会选置信度最高的帧，但这是错误的。置信度高只代表"模型确定这是个人"，不代表"这个人拍得清楚"。一个背对镜头、远离画面的人可能置信度很高，但毫无识别价值。

### 2.3 物理特征计算：Atomic 8 战术仪表盘

> 💡 **核心洞察**：很多问题可以用物理规则直接回答，不需要动用昂贵的 VLM。

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                        Atomic 8 物理特征协议                              │
│                        ─────────────────────                              │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │                                                                 │   │
│   │   📍 时空定位                                                   │   │
│   │   ├─ start_s     首次出现时间（秒）   "最早出现的人"            │   │
│   │   ├─ end_s       最后出现时间（秒）   "最后离开的人"            │   │
│   │   └─ duration_s  停留时长（秒）       "停留超过30秒的人"        │   │
│   │                                                                 │   │
│   │   📐 几何轨迹                                                   │   │
│   │   ├─ centroids         归一化中心点序列 (0~1)                   │   │
│   │   └─ displacement_vec  首尾位移向量    "从左往右走的人"         │   │
│   │                                                                 │   │
│   │   🏃 运动语义                                                   │   │
│   │   ├─ norm_speed    归一化速度（身高/秒）                        │   │
│   │   │                ├─ < 0.1  静止/几乎不动                      │   │
│   │   │                ├─ 0.1~1.8 正常行走                          │   │
│   │   │                └─ > 1.8   跑步/快速移动                     │   │
│   │   │                                                             │   │
│   │   ├─ linearity     轨迹线性度 (0~1)                             │   │
│   │   │                ├─ < 0.3  徘徊/打转                          │   │
│   │   │                └─ > 0.7  直线通过                           │   │
│   │   │                                                             │   │
│   │   └─ scale_change  尺度变化比                                   │   │
│   │                    ├─ > 1.2  正在靠近镜头                       │   │
│   │                    └─ < 0.8  正在远离镜头                       │   │
│   │                                                                 │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│   💡 设计原则：                                                          │
│   ├─ 只存储纯几何量，不存储任何语义标签                                  │
│   ├─ "是不是小偷"、"是不是在打架" 这些判断交给上层                       │
│   └─ 所有行为语义都在这些原子事实之上推导                                │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

**为什么叫"仪表盘"？**

就像飞行员不需要看窗外就能知道飞机的高度、速度、方向一样，我们把这些物理数据预先算好，后续的 VLM 可以直接读取仪表盘，而不需要自己去"看"视频来估算速度。

### 2.4 证据包：统一的数据交换格式

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                         EvidencePackage                                  │
│                         证据包 / 嫌疑人档案                               │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │                                                                 │   │
│   │   🎯 身份信息                                                   │   │
│   │   ├─ video_id      来自哪个视频                                 │   │
│   │   ├─ track_id      这个人的唯一编号                             │   │
│   │   └─ video_path    原视频路径（用于回溯取证）                    │   │
│   │                                                                 │   │
│   │   📍 时空轨迹                                                   │   │
│   │   ├─ frames        出现在哪些帧 [1, 2, 3, ..., 900]            │   │
│   │   ├─ bboxes        每帧的检测框 [(x1,y1,x2,y2), ...]           │   │
│   │   └─ fps           帧率（用于帧号↔时间转换）                    │   │
│   │                                                                 │   │
│   │   📸 视觉证据                                                   │   │
│   │   ├─ crops         所有裁剪图路径                               │   │
│   │   └─ best_bbox_index  最佳截图的索引                            │   │
│   │                                                                 │   │
│   │   📊 战术仪表盘                                                  │   │
│   │   └─ features      Atomic 8 物理特征                            │   │
│   │                                                                 │   │
│   │   🧮 派生属性（自动计算）                                        │   │
│   │   ├─ start_time_seconds   起始时间 = start_frame / fps         │   │
│   │   └─ end_time_seconds     结束时间 = end_frame / fps           │   │
│   │                                                                 │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│   💡 设计意图：                                                          │
│   这是整个系统的"数据货币"。                                             │
│   所有下游模块（召回、VLM、可视化）只需要拿到 EvidencePackage，          │
│   不需要知道数据从哪来、怎么算的。                                       │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

---

## 三、第一道防线：CLIP 快速安检

### The CLIP Recall Layer

这是 Phase 2.5+ 新增的 **"极速排除"** 环节。它的任务极其简单：

> **只看外观，秒杀无关人员。**

### 3.1 工作机制

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│   用户查询："找穿蓝衣服的人"                                              │
│        │                                                                 │
│        ▼                                                                 │
│   ┌──────────────────────────────────────────────────────────────────┐  │
│   │                      CLIP / SigLIP                               │  │
│   │                      ～400M 参数的轻量模型                        │  │
│   ├──────────────────────────────────────────────────────────────────┤  │
│   │                                                                  │  │
│   │   文本编码器                        图像编码器                    │  │
│   │   ┌──────────┐                     ┌──────────┐                 │  │
│   │   │ "蓝衣服" │                     │  证件照   │ × N 个人        │  │
│   │   └────┬─────┘                     └────┬─────┘                 │  │
│   │        │                                │                        │  │
│   │        ▼                                ▼                        │  │
│   │   [文本向量]                       [图像向量] × N                │  │
│   │        │                                │                        │  │
│   │        └────────────────┬───────────────┘                        │  │
│   │                         │                                        │  │
│   │                         ▼                                        │  │
│   │                  余弦相似度计算                                   │  │
│   │                  cos(text, image)                                │  │
│   │                                                                  │  │
│   └──────────────────────────────────────────────────────────────────┘  │
│        │                                                                 │
│        ▼                                                                 │
│   ┌──────────────────────────────────────────────────────────────────┐  │
│   │                                                                  │  │
│   │   ID=1  相似度=0.42  ✅ 保留 → 看着确实像蓝色                    │  │
│   │   ID=2  相似度=0.08  ❌ 排除 → 这人穿的是红色                    │  │
│   │   ID=3  相似度=0.31  ✅ 保留 → 可能是深蓝/灰蓝                   │  │
│   │   ID=4  相似度=0.05  ❌ 排除 → 黑衣服，完全不沾边                │  │
│   │   ID=5  相似度=0.38  ✅ 保留 → 蓝色牛仔裤？                      │  │
│   │   ...                                                            │  │
│   │                                                                  │  │
│   └──────────────────────────────────────────────────────────────────┘  │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

### 3.2 红线过滤逻辑

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                            红线过滤策略                                   │
│                                                                          │
│   ┌────────────────────────────────────────────────────────────────┐    │
│   │                                                                │    │
│   │   相似度 < 红线阈值 (例如 0.05~0.20)                           │    │
│   │         │                                                      │    │
│   │         ▼                                                      │    │
│   │   ┌──────────────────────────────────────────────────────┐    │    │
│   │   │                                                      │    │    │
│   │   │   判定：与查询描述"毫无关系"                          │    │    │
│   │   │                                                      │    │    │
│   │   │   处置：直接标记为"排除"                             │    │    │
│   │   │                                                      │    │    │
│   │   │   结果：永远不会进入昂贵的 VLM 环节                   │    │    │
│   │   │                                                      │    │    │
│   │   └──────────────────────────────────────────────────────┘    │    │
│   │                                                                │    │
│   └────────────────────────────────────────────────────────────────┘    │
│                                                                          │
│   💡 战略意义：                                                          │
│   ├─ 一次 CLIP 推理耗时：～10ms                                         │
│   ├─ 一次 VLM 推理耗时：～2000ms                                        │
│   ├─ 如果能过滤掉 90% 的人，整体延迟降低 10 倍                          │
│   └─ 这一步是系统吞吐量的关键瓶颈点                                     │
│                                                                          │
│   ⚠️ 风险控制：                                                         │
│   ├─ 红线阈值不能设太高，否则会误杀真正的匹配                            │
│   ├─ 宁可让一些"可能相关"的人进入 VLM，也不能漏掉真正的目标              │
│   └─ 这是一个"召回优先"的环节，精度交给 VLM 来保证                       │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

### 3.3 可选开关

CLIP 过滤默认关闭（`enable_clip_filter=False`），原因：

1. **小数据集下收益有限**：如果只有 10 个人，过滤掉 9 个省不了多少时间
2. **避免误杀**：CLIP 在某些 edge case 下可能误判（如蓝灰色被判为灰色）
3. **安全第一**：在生产环境验证稳定后再启用

---

## 四、智能调度：战术分发

### The Smart Router

经过 CLIP 筛选后，剩下的都是"看着像"的嫌疑人。

现在，我们需要决定：**用什么级别的火力去审讯他们？**

这个决策由 **Router（大脑）** 根据用户的问题来自动判定。

### 4.1 核心判别逻辑

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                        Router 决策树                                     │
│                                                                          │
│   用户问题                                                               │
│       │                                                                  │
│       ▼                                                                  │
│   ┌───────────────────────────────────────────────────────────────┐     │
│   │                     语义分析                                  │     │
│   │                                                               │     │
│   │   问题里有没有"动作词"？                                      │     │
│   │   ├─ 跑、走、移动、离开、进入、靠近、徘徊、停留...            │     │
│   │   └─ 如果有 → need_context = true                            │     │
│   │                                                               │     │
│   │   问题里有没有"方向词"？                                      │     │
│   │   ├─ 往左、往右、朝着、从...到...                            │     │
│   │   └─ 如果有 → need_context = true                            │     │
│   │                                                               │     │
│   │   问题里有没有"环境词"？                                      │     │
│   │   ├─ 商店、门口、收银台、入口...                             │     │
│   │   └─ 如果有 → need_context = true                            │     │
│   │                                                               │     │
│   │   以上都没有？                                                │     │
│   │   └─ need_context = false（纯外观查询）                       │     │
│   │                                                               │     │
│   └───────────────────────────────────────────────────────────────┘     │
│       │                                                                  │
│       ├─────────────────────────┬────────────────────────────┐          │
│       │                         │                            │          │
│       ▼                         ▼                            ▼          │
│                                                                          │
│   need_context = false     need_context = true                          │
│   ┌─────────────────┐      ┌─────────────────────────────────┐          │
│   │                 │      │                                 │          │
│   │  "穿蓝衣服的人"  │      │  "从商店跑出来的人"             │          │
│   │  "背书包的男孩"  │      │  "在门口徘徊的人"               │          │
│   │  "戴眼镜的女人"  │      │  "往左边走的蓝衣服"             │          │
│   │                 │      │                                 │          │
│   └────────┬────────┘      └───────────────┬─────────────────┘          │
│            │                               │                             │
│            ▼                               ▼                             │
│                                                                          │
│      下发指令：                      下发指令：                          │
│      Layer 1                         Layer 2                             │
│      特写模式                        全景模式                            │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

### 4.2 ExecutionPlan：可执行计划

Router 的输出不是简单的 true/false，而是一份完整的 **执行计划**：

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                       ExecutionPlan 执行计划                             │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │                                                                 │   │
│   │   description: "person in blue shirt running left"              │   │
│   │   ├─ 用于 CLIP 召回的英文描述                                   │   │
│   │   └─ 简化版，突出视觉关键词                                     │   │
│   │                                                                 │   │
│   │   visual_tags: ["blue shirt", "running"]                        │   │
│   │   ├─ 更细粒度的标签列表                                         │   │
│   │   └─ 可与 description 拼接使用                                  │   │
│   │                                                                 │   │
│   │   hard_rules: {                                                 │   │
│   │     norm_speed: {min: 1.8},    ← 速度约束（跑步 > 1.8）         │   │
│   │     scale_change: {min: 1.2}   ← 深度约束（正在靠近）           │   │
│   │   }                                                             │   │
│   │   ├─ 基于 Atomic 8 的物理过滤规则                               │   │
│   │   └─ 如果查询不涉及运动，则为空 {}                              │   │
│   │                                                                 │   │
│   │   meta: {                                                       │   │
│   │     need_context: true         ← 关键开关                       │   │
│   │   }                                                             │   │
│   │   └─ 决定走 Layer 1 还是 Layer 2                                │   │
│   │                                                                 │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

### 4.3 约束清洗：防止幻觉

Router 使用的是 VLM（Qwen3-VL），它可能会"过度解读"用户的问题。

例如用户只说"找穿蓝衣服的人"，VLM 可能幻觉出 `{norm_speed: {min: 0.8, max: 1.2}}`。

**清洗策略**：

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                          约束清洗逻辑                                    │
│                                                                          │
│   ┌────────────────────────────────────────────────────────────────┐    │
│   │                                                                │    │
│   │   Step 1: 关键词检测                                          │    │
│   │                                                                │    │
│   │   motion_keywords = ["run", "walk", "fast", "slow",           │    │
│   │                      "move", "leave", "approach", "wander"]    │    │
│   │                                                                │    │
│   │   如果用户问题中不包含任何动作词：                             │    │
│   │   └─ 清空所有 hard_rules → 不做任何物理过滤                   │    │
│   │                                                                │    │
│   └────────────────────────────────────────────────────────────────┘    │
│                                                                          │
│   ┌────────────────────────────────────────────────────────────────┐    │
│   │                                                                │    │
│   │   Step 2: 幻觉检测                                            │    │
│   │                                                                │    │
│   │   VLM 常见幻觉：scale_change: {min: 0.8, max: 1.2}            │    │
│   │   ├─ 这是"既不靠近也不远离"的默认值                           │    │
│   │   └─ 毫无意义，但 VLM 总喜欢输出                              │    │
│   │                                                                │    │
│   │   检测到这种模式 → 直接删除该约束                              │    │
│   │                                                                │    │
│   └────────────────────────────────────────────────────────────────┘    │
│                                                                          │
│   💡 设计意图：                                                          │
│   ├─ VLM 是强大的，但也是"话痨"的                                       │
│   ├─ 我们需要一个兜底机制来防止它的过度发挥                              │
│   └─ 代码比 VLM 更可靠，用代码来约束 VLM                                │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

---

## 五、核心审讯：VLM 双轨验证

### The Dual-Track VLM Verifier

这是系统的 **最后一道防线**，由 **Qwen3-VL-4B** 坐镇，执行 Router 下发的战术。

### 5.1 双轨设计哲学

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                    为什么需要两条轨道？                                   │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │                                                                 │   │
│   │   问题类型不同，需要的"证据"也不同                               │   │
│   │                                                                 │   │
│   │   ┌───────────────────────┐  ┌───────────────────────────────┐ │   │
│   │   │     "找人"类问题      │  │       "找动作"类问题           │ │   │
│   │   ├───────────────────────┤  ├───────────────────────────────┤ │   │
│   │   │                       │  │                               │ │   │
│   │   │  穿什么颜色？          │  │  在跑还是在走？                │ │   │
│   │   │  背什么包？            │  │  往哪个方向？                  │ │   │
│   │   │  戴什么帽子？          │  │  是进入还是离开？              │ │   │
│   │   │  什么发型？            │  │  有没有在徘徊？                │ │   │
│   │   │                       │  │                               │ │   │
│   │   │  需要：高清特写        │  │  需要：时间序列 + 环境背景     │ │   │
│   │   │  不需要：环境信息      │  │  不需要：极致细节              │ │   │
│   │   │                       │  │                               │ │   │
│   │   └───────────────────────┘  └───────────────────────────────┘ │   │
│   │             │                              │                    │   │
│   │             ▼                              ▼                    │   │
│   │        Layer 1                        Layer 2                   │   │
│   │        特写模式                        全景模式                  │   │
│   │                                                                 │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│   💡 核心洞察：                                                          │
│   ├─ 一张图胜过千言万语，但不是任何图都行                                │
│   ├─ 问"衣服颜色" → 需要看得清纤维纹理的特写                            │
│   ├─ 问"运动方向" → 需要看到背景参照物 + 时间变化                       │
│   └─ 用错误的证据问问题 = 垃圾进垃圾出                                  │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

### 5.2 Layer 1：特写审讯模式

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                   Layer 1: Crop Mode 特写模式                            │
│                                                                          │
│   适用场景：Router 判定为 need_context = false（纯外观查询）              │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │                                                                 │   │
│   │   输入：仅一张高清"证件照"                                      │   │
│   │                                                                 │   │
│   │   ┌─────────────────────────────────────────────────────────┐  │   │
│   │   │                                                         │  │   │
│   │   │                    ┌───────────────┐                    │  │   │
│   │   │                    │               │                    │  │   │
│   │   │                    │   📸 特写图    │  ← 从原图直接裁剪  │  │   │
│   │   │                    │   高分辨率     │     像素无损失    │  │   │
│   │   │                    │   细节清晰     │                    │  │   │
│   │   │                    │               │                    │  │   │
│   │   │                    └───────────────┘                    │  │   │
│   │   │                                                         │  │   │
│   │   │   VLM 视角：                                            │  │   │
│   │   │   └─ 拿着放大镜看这张图                                 │  │   │
│   │   │   └─ 能看清：衣服Logo、眼镜款式、发型、配饰...         │  │   │
│   │   │                                                         │  │   │
│   │   └─────────────────────────────────────────────────────────┘  │   │
│   │                                                                 │   │
│   │   Prompt 结构：                                                 │   │
│   │   ┌─────────────────────────────────────────────────────────┐  │   │
│   │   │ Query: "找穿蓝衣服的人"                                  │  │   │
│   │   │                                                         │  │   │
│   │   │ 请根据这张图片判断：这个人是否符合描述？                  │  │   │
│   │   │                                                         │  │   │
│   │   │ 请用 JSON 格式回答：                                     │  │   │
│   │   │ {"match": true/false, "reason": "...", "confidence": N} │  │   │
│   │   └─────────────────────────────────────────────────────────┘  │   │
│   │                                                                 │   │
│   │   输出：是/否 + 理由                                           │   │
│   │                                                                 │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│   ⚡ 优势：                                                              │
│   ├─ 极快：只处理一张小图                                               │
│   ├─ 极准：高分辨率 = 细节无遗漏                                        │
│   └─ 极省：Token 消耗最小化                                             │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

### 5.3 Layer 2：全景时空审讯模式

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│               Layer 2: Full-Frame Context Mode 全景模式                  │
│                                                                          │
│   适用场景：Router 判定为 need_context = true（涉及运动/环境）            │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │                                                                 │   │
│   │   输入组合拳：三位一体                                          │   │
│   │                                                                 │   │
│   │   ① 通缉令 (Reference Crop)                                    │   │
│   │   ┌───────────────────────────────────────────────────────┐    │   │
│   │   │                                                       │    │   │
│   │   │       ┌───────────┐                                   │    │   │
│   │   │       │ 📸 证件照  │ ← "我们要找的人长这样"            │    │   │
│   │   │       └───────────┘                                   │    │   │
│   │   │                                                       │    │   │
│   │   │   作用：让 VLM 先记住目标的外观                        │    │   │
│   │   │         避免在全景图里"找错人"                         │    │   │
│   │   │                                                       │    │   │
│   │   └───────────────────────────────────────────────────────┘    │   │
│   │                                                                 │   │
│   │   ② 监控录像 + 动态红框 (Context Stream + Burn-in)              │   │
│   │   ┌───────────────────────────────────────────────────────┐    │   │
│   │   │                                                       │    │   │
│   │   │   🎞️ Filmstrip 时间线胶片                              │    │   │
│   │   │                                                       │    │   │
│   │   │   t=0       t=1       t=2       t=3       t=4         │    │   │
│   │   │   ┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐    │    │   │
│   │   │   │     │   │     │   │     │   │     │   │     │    │    │   │
│   │   │   │ 🔴  │   │  🔴 │   │  🔴 │   │   🔴│   │   🔴│    │    │   │
│   │   │   │ ID:1│   │ ID:1│   │ ID:1│   │ ID:1│   │ ID:1│    │    │   │
│   │   │   │     │   │     │   │     │   │     │   │     │    │    │   │
│   │   │   └─────┘   └─────┘   └─────┘   └─────┘   └─────┘    │    │   │
│   │   │                                                       │    │   │
│   │   │   ← 过去 ─────────────────────────────── 未来 →       │    │   │
│   │   │                                                       │    │   │
│   │   │   🔴 红框：在每帧的准确位置画上醒目的红框              │    │   │
│   │   │           像探照灯一样锁定目标                         │    │   │
│   │   │           防止 VLM 在复杂场景中迷失                    │    │   │
│   │   │                                                       │    │   │
│   │   │   时序理解：VLM 会自动理解帧的顺序                     │    │   │
│   │   │            从红框的位置变化推断运动方向                 │    │   │
│   │   │                                                       │    │   │
│   │   └───────────────────────────────────────────────────────┘    │   │
│   │                                                                 │   │
│   │   ③ 战术仪表盘 (Telemetry)                                     │   │
│   │   ┌───────────────────────────────────────────────────────┐    │   │
│   │   │                                                       │    │   │
│   │   │   Target ID: 1                                        │    │   │
│   │   │   ├─ Motion: Running/fast (norm_speed 2.1)           │    │   │
│   │   │   ├─ Direction: Moving left                          │    │   │
│   │   │   ├─ Path: Direct (high linearity)                   │    │   │
│   │   │   └─ Depth: Leaving camera (scale decreasing)        │    │   │
│   │   │                                                       │    │   │
│   │   │   作用：把预先算好的物理数据直接告诉 VLM               │    │   │
│   │   │         VLM 不需要自己"猜"速度是快是慢                 │    │   │
│   │   │                                                       │    │   │
│   │   └───────────────────────────────────────────────────────┘    │   │
│   │                                                                 │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│   VLM 的判断过程：                                                       │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │                                                                 │   │
│   │   1. 看通缉令 → 记住目标长什么样                                │   │
│   │                                                                 │   │
│   │   2. 看 Filmstrip → 找到被红框标注的那个人                     │   │
│   │                    → 观察红框的移动轨迹                         │   │
│   │                    → 结合背景判断位置（在商店门口？收银台旁？） │   │
│   │                                                                 │   │
│   │   3. 读仪表盘 → 确认物理数据（速度确实很快、确实在往左走）      │   │
│   │                                                                 │   │
│   │   4. 综合裁决 → 这个人是否符合"从商店跑出来往左走"？            │   │
│   │                                                                 │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

### 5.4 Filmstrip 的设计精髓

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                   Filmstrip：时间 → 空间的魔法映射                       │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │                                                                 │   │
│   │   问题：Qwen3-VL 是"非视频模型"，它不能处理视频                 │   │
│   │                                                                 │   │
│   │   ┌───────────────────────────────────────────────────────┐    │   │
│   │   │                                                       │    │   │
│   │   │   视频 = 时间维度 + 空间维度                          │    │   │
│   │   │                                                       │    │   │
│   │   │   但 VLM 只能看"一张图"                               │    │   │
│   │   │                                                       │    │   │
│   │   │   怎么让它理解"时间流逝"？                            │    │   │
│   │   │                                                       │    │   │
│   │   └───────────────────────────────────────────────────────┘    │   │
│   │                                                                 │   │
│   │   解决方案：把时间"拍扁"成空间                                  │   │
│   │                                                                 │   │
│   │   ┌───────────────────────────────────────────────────────┐    │   │
│   │   │                                                       │    │   │
│   │   │   原始视频：                                          │    │   │
│   │   │   ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐                      │    │   │
│   │   │   │ 1 │→│ 2 │→│ 3 │→│ 4 │→│ 5 │  时间顺序播放       │    │   │
│   │   │   └───┘ └───┘ └───┘ └───┘ └───┘                      │    │   │
│   │   │                                                       │    │   │
│   │   │                     ↓ 胶片拼接                        │    │   │
│   │   │                                                       │    │   │
│   │   │   Filmstrip：                                         │    │   │
│   │   │   ┌───┬───┬───┬───┬───┐                               │    │   │
│   │   │   │ 1 │ 2 │ 3 │ 4 │ 5 │  空间并排                    │    │   │
│   │   │   └───┴───┴───┴───┴───┘                               │    │   │
│   │   │   ← 左=过去     右=未来 →                              │    │   │
│   │   │                                                       │    │   │
│   │   └───────────────────────────────────────────────────────┘    │   │
│   │                                                                 │   │
│   │   VLM 的理解方式：                                              │   │
│   │                                                                 │   │
│   │   ┌───────────────────────────────────────────────────────┐    │   │
│   │   │                                                       │    │   │
│   │   │   "红框在左边 → 红框在右边"                           │    │   │
│   │   │        ↓                                              │    │   │
│   │   │   "这个人从左往右走"                                  │    │   │
│   │   │                                                       │    │   │
│   │   │   "红框变大 → 红框变大 → 红框变大"                    │    │   │
│   │   │        ↓                                              │    │   │
│   │   │   "这个人在靠近镜头"                                  │    │   │
│   │   │                                                       │    │   │
│   │   │   "红框位置几乎不变，但帧在变"                        │    │   │
│   │   │        ↓                                              │    │   │
│   │   │   "这个人在原地徘徊"                                  │    │   │
│   │   │                                                       │    │   │
│   │   └───────────────────────────────────────────────────────┘    │   │
│   │                                                                 │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│   💡 这就是 Phase 2.5 的核心创新：                                       │
│      用空间位置编码时间信息，让静态模型"看懂"动态视频                    │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

### 5.5 健壮解析：防止输出崩溃

VLM 的输出可能是不完整的（Token 被截断）或格式错误的。

我们需要一个健壮的解析器来"抢救"尽可能多的结果：

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                        健壮解析策略                                       │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │                                                                 │   │
│   │   VLM 原始输出（可能被截断）：                                   │   │
│   │                                                                 │   │
│   │   {                                                             │   │
│   │     "1": {"match": true, "reason": "blue shirt", "conf": 0.9}, │   │
│   │     "2": {"match": false, "reason": "red shirt"},              │   │
│   │     "3": {"match": true, "reason": "looks blue bu             │   │
│   │   ← 输出被截断，JSON 不完整                                     │   │
│   │                                                                 │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │                                                                 │   │
│   │   解析策略：逐 ID 抽取                                          │   │
│   │                                                                 │   │
│   │   for each expected_id:                                         │   │
│   │       1. 用正则找到 "id": {...} 块                              │   │
│   │       2. 尝试解析这个独立的 JSON 块                              │   │
│   │       3. 成功 → 提取结果                                        │   │
│   │       4. 失败 → 标记为"解析失败"                                │   │
│   │                                                                 │   │
│   │   结果：                                                        │   │
│   │   ├─ ID=1: ✅ 成功解析，match=true                              │   │
│   │   ├─ ID=2: ✅ 成功解析，match=false                             │   │
│   │   └─ ID=3: ⚠️ 解析失败（被截断），标记为 error                  │   │
│   │                                                                 │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│   💡 设计意图：                                                          │
│   ├─ 宁可返回部分结果，也不要整体失败                                   │
│   ├─ 解析失败的 ID 可以重试或用其他方式处理                             │
│   └─ 这在批量处理时尤其重要（一次送 5 个人）                            │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

---

## 六、系统流转全景图

```
┌──────────────────────────────────────────────────────────────────────────────────────┐
│                                                                                      │
│                              Edge-Detective 完整流程                                 │
│                                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐│
│  │                          离线阶段：建立档案库                                    ││
│  │                                                                                 ││
│  │   📹 视频                                                                       ││
│  │    │                                                                            ││
│  │    ▼                                                                            ││
│  │   ┌──────────┐      ┌──────────┐      ┌──────────┐      ┌──────────┐           ││
│  │   │  YOLO    │ ───→ │ByteTrack │ ───→ │ Features │ ───→ │ Evidence │           ││
│  │   │  检测    │      │  跟踪    │      │ Atomic 8 │      │  打包    │           ││
│  │   └──────────┘      └──────────┘      └──────────┘      └──────────┘           ││
│  │                                                               │                 ││
│  │                                                               ▼                 ││
│  │                                              💾 semantic_database.json         ││
│  │                                              📁 crops/ 裁剪图                   ││
│  │                                                                                 ││
│  └─────────────────────────────────────────────────────────────────────────────────┘│
│                                              │                                       │
│                                              │ 索引一次，查询多次                    │
│                                              ▼                                       │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐│
│  │                          在线阶段：问题驱动检索                                  ││
│  │                                                                                 ││
│  │   💬 "找穿蓝衣服往左跑的人"                                                      ││
│  │    │                                                                            ││
│  │    ▼                                                                            ││
│  │   ┌──────────────────────────────────────────────────────────────────────────┐ ││
│  │   │  Router (VLM)                                                            │ ││
│  │   │  ├─ 语义分析："蓝衣服"(外观) + "往左跑"(运动)                             │ ││
│  │   │  ├─ 生成 ExecutionPlan                                                   │ ││
│  │   │  └─ 判定：need_context = true → 走 Layer 2                               │ ││
│  │   └──────────────────────────────────────────────────────────────────────────┘ ││
│  │    │                                                                            ││
│  │    ▼                                                                            ││
│  │   ┌──────────────────────────────────────────────────────────────────────────┐ ││
│  │   │  Recall Engine                                                           │ ││
│  │   │  └─ 返回所有轨迹（Phase 1 简化版）                                        │ ││
│  │   └──────────────────────────────────────────────────────────────────────────┘ ││
│  │    │                                                                            ││
│  │    ▼                                                                            ││
│  │   ┌──────────────────────────────────────────────────────────────────────────┐ ││
│  │   │  CLIP Filter (可选，默认关闭)                                             │ ││
│  │   │  └─ 用"蓝衣服"做快速筛选，剔除明显不相关的人                               │ ││
│  │   └──────────────────────────────────────────────────────────────────────────┘ ││
│  │    │                                                                            ││
│  │    ▼                                                                            ││
│  │   ┌──────────────────────────────────────────────────────────────────────────┐ ││
│  │   │  VLM Verifier (Layer 2)                                                  │ ││
│  │   │  ├─ 输入：证件照 + Filmstrip + 仪表盘                                    │ ││
│  │   │  ├─ 判断：外观是否蓝色？运动是否往左？速度是否在跑？                       │ ││
│  │   │  └─ 输出：match=true/false + reason + confidence                        │ ││
│  │   └──────────────────────────────────────────────────────────────────────────┘ ││
│  │    │                                                                            ││
│  │    ▼                                                                            ││
│  │   ┌──────────────────────────────────────────────────────────────────────────┐ ││
│  │   │  Output                                                                  │ ││
│  │   │  ├─ QueryResult: track_id, start_s, end_s, reason                        │ ││
│  │   │  └─ tracking_*.mp4: 高亮视频                                             │ ││
│  │   └──────────────────────────────────────────────────────────────────────────┘ ││
│  │                                                                                 ││
│  └─────────────────────────────────────────────────────────────────────────────────┘│
│                                                                                      │
└──────────────────────────────────────────────────────────────────────────────────────┘
```

---

## 七、设计决策总结

| 决策点 | 选择 | 理由 |
|--------|------|------|
| 检测模型 | YOLO11n | 轻量、快速、精度足够 |
| 跟踪算法 | ByteTrack | 工业级稳定性，遮挡处理好 |
| 最佳裁剪 | 面积+中心性 | 比置信度更可靠地选出清晰照片 |
| 物理特征 | 预计算 Atomic 8 | 让 VLM 读数据而非"猜"数据 |
| 召回策略 | CLIP 快筛(可选) | 10x 吞吐量提升 |
| 路由分发 | VLM + 规则兜底 | VLM 理解语义，规则防止幻觉 |
| 验证分层 | Layer 1/2 双轨 | 按需选择火力，避免过度消耗 |
| 时序理解 | Filmstrip | 让非视频模型理解运动 |
| 结果解析 | 逐 ID 抽取 | 健壮处理截断输出 |

---

## 八、技术栈

```
┌──────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│   🤖 核心模型                                                            │
│   ├─ Qwen3-VL-4B-Instruct     视觉语言模型（Router + Verifier）          │
│   ├─ YOLO11n                  目标检测                                   │
│   ├─ ByteTrack                多目标跟踪                                 │
│   └─ SigLIP/CLIP (可选)       视觉向量召回                               │
│                                                                          │
│   🚀 推理框架                                                            │
│   └─ vLLM                     高性能 LLM 推理服务                        │
│                                                                          │
│   🐍 核心依赖                                                            │
│   ├─ openai                   vLLM OpenAI 兼容接口                       │
│   ├─ opencv-python            视频处理                                   │
│   ├─ ultralytics              YOLO                                       │
│   └─ numpy                    数值计算                                   │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

---

> 💡 **一句话总结**：  
> Edge-Detective 通过 **三层漏斗 + 双轨验证 + Filmstrip 时空映射**，  
> 让 4B 参数的非视频 VLM 也能精准地 **"在视频中找人、找动作"**。
