# ğŸ­ Phase 2: ç”Ÿäº§å¯ç”¨ å¼€å‘è®¡åˆ’

> **ç›®æ ‡**: æ€§èƒ½ä¼˜åŒ– + ç¨³å®šéƒ¨ç½² + é‡åŒ–æ¨¡å‹æ”¯æŒ  
> **å‘¨æœŸ**: 3-4 å‘¨  
> **å‰ç½®æ¡ä»¶**: Phase 1 å®Œæˆ

---

## ç›®å½•

1. [Week 4: å‘é‡åº“ + å¯¹è±¡å­˜å‚¨](#week-4-å‘é‡åº“--å¯¹è±¡å­˜å‚¨)
2. [Week 5: é‡åŒ–æ¨¡å‹ + ModelRegistry](#week-5-é‡åŒ–æ¨¡å‹--modelregistry)
3. [Week 6: å‰ç«¯éª¨æ¶](#week-6-å‰ç«¯éª¨æ¶)
4. [Week 7: å®Œæ•´æµ‹è¯• + æ€§èƒ½ä¼˜åŒ–](#week-7-å®Œæ•´æµ‹è¯•--æ€§èƒ½ä¼˜åŒ–)
5. [å¾…åˆ é™¤æ–‡ä»¶æ¸…å•](#å¾…åˆ é™¤æ–‡ä»¶æ¸…å•)
6. [éªŒæ”¶æ ‡å‡†](#éªŒæ”¶æ ‡å‡†)

---

## Week 4: å‘é‡åº“ + å¯¹è±¡å­˜å‚¨

### Day 1-2: Qdrant å‘é‡åº“é›†æˆ

#### éœ€è¦åˆ›å»ºçš„æ–‡ä»¶

```bash
src/ports/vector_store_port.py
src/adapters/vector/__init__.py
src/adapters/vector/qdrant_adapter.py
```

#### ä»£ç å®ç°

**`src/ports/vector_store_port.py`**
```python
"""å‘é‡å­˜å‚¨ç«¯å£"""
from typing import Protocol, List


class VectorStorePort(Protocol):
    """å‘é‡å­˜å‚¨æŠ½è±¡æ¥å£"""
    
    async def upsert(
        self,
        video_id: str,
        track_id: int,
        embedding: List[float],
        metadata: dict
    ) -> None:
        """æ’å…¥/æ›´æ–°å‘é‡"""
        ...
    
    async def search(
        self,
        video_id: str,
        query_vector: List[float],
        top_k: int = 50
    ) -> List[dict]:
        """å‘é‡æ£€ç´¢"""
        ...
    
    async def delete_video(self, video_id: str) -> None:
        """åˆ é™¤è§†é¢‘çš„æ‰€æœ‰å‘é‡"""
        ...
```

**`src/adapters/vector/qdrant_adapter.py`**
```python
"""Qdrant å‘é‡åº“é€‚é…å™¨"""
from typing import List
from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance, VectorParams, PointStruct
)

from ports.vector_store_port import VectorStorePort


class QdrantAdapter:
    """Qdrant å‘é‡å­˜å‚¨å®ç°"""
    
    COLLECTION_PREFIX = "track_embeddings"
    VECTOR_DIM = 768  # SigLIP
    
    def __init__(self, host: str = "localhost", port: int = 6333):
        self.client = QdrantClient(host=host, port=port)
    
    def _collection_name(self, video_id: str) -> str:
        return f"{self.COLLECTION_PREFIX}_{video_id}"
    
    async def ensure_collection(self, video_id: str):
        name = self._collection_name(video_id)
        collections = self.client.get_collections().collections
        if name not in [c.name for c in collections]:
            self.client.create_collection(
                collection_name=name,
                vectors_config=VectorParams(
                    size=self.VECTOR_DIM,
                    distance=Distance.COSINE
                )
            )
    
    async def upsert(
        self,
        video_id: str,
        track_id: int,
        embedding: List[float],
        metadata: dict
    ):
        await self.ensure_collection(video_id)
        self.client.upsert(
            collection_name=self._collection_name(video_id),
            points=[PointStruct(
                id=track_id,
                vector=embedding,
                payload={"video_id": video_id, "track_id": track_id, **metadata}
            )]
        )
    
    async def search(
        self,
        video_id: str,
        query_vector: List[float],
        top_k: int = 50
    ) -> List[dict]:
        results = self.client.search(
            collection_name=self._collection_name(video_id),
            query_vector=query_vector,
            limit=top_k,
        )
        return [{"track_id": h.id, "score": h.score, **h.payload} for h in results]
    
    async def delete_video(self, video_id: str):
        name = self._collection_name(video_id)
        if self.client.collection_exists(name):
            self.client.delete_collection(name)
```

### Day 3-4: MinIO å¯¹è±¡å­˜å‚¨

**`src/ports/storage_port.py`**
```python
"""å¯¹è±¡å­˜å‚¨ç«¯å£"""
from typing import Protocol, BinaryIO


class ObjectStoragePort(Protocol):
    """å¯¹è±¡å­˜å‚¨æŠ½è±¡æ¥å£"""
    
    async def upload(
        self,
        bucket: str,
        key: str,
        data: BinaryIO,
        content_type: str = "application/octet-stream"
    ) -> str:
        """ä¸Šä¼ æ–‡ä»¶ï¼Œè¿”å› URL"""
        ...
    
    async def download(self, bucket: str, key: str) -> bytes:
        """ä¸‹è½½æ–‡ä»¶"""
        ...
    
    async def get_presigned_url(
        self,
        bucket: str,
        key: str,
        expires: int = 3600
    ) -> str:
        """è·å–é¢„ç­¾å URL"""
        ...
    
    async def delete(self, bucket: str, key: str) -> None:
        """åˆ é™¤æ–‡ä»¶"""
        ...
```

**`src/adapters/storage/minio_adapter.py`**
```python
"""MinIO å¯¹è±¡å­˜å‚¨é€‚é…å™¨"""
import io
from datetime import timedelta
from typing import BinaryIO

from minio import Minio

from ports.storage_port import ObjectStoragePort


class MinioAdapter:
    """MinIO å¯¹è±¡å­˜å‚¨å®ç°"""
    
    def __init__(
        self,
        endpoint: str = "localhost:9000",
        access_key: str = "minioadmin",
        secret_key: str = "minioadmin",
        secure: bool = False
    ):
        self.client = Minio(
            endpoint,
            access_key=access_key,
            secret_key=secret_key,
            secure=secure
        )
    
    async def ensure_bucket(self, bucket: str):
        if not self.client.bucket_exists(bucket):
            self.client.make_bucket(bucket)
    
    async def upload(
        self,
        bucket: str,
        key: str,
        data: BinaryIO,
        content_type: str = "application/octet-stream"
    ) -> str:
        await self.ensure_bucket(bucket)
        data.seek(0, 2)
        size = data.tell()
        data.seek(0)
        self.client.put_object(bucket, key, data, size, content_type)
        return f"s3://{bucket}/{key}"
    
    async def download(self, bucket: str, key: str) -> bytes:
        response = self.client.get_object(bucket, key)
        return response.read()
    
    async def get_presigned_url(
        self,
        bucket: str,
        key: str,
        expires: int = 3600
    ) -> str:
        return self.client.presigned_get_object(
            bucket, key, expires=timedelta(seconds=expires)
        )
    
    async def delete(self, bucket: str, key: str):
        self.client.remove_object(bucket, key)
```

### Day 5: Docker Compose æ•´åˆ

**`docker-compose.yml`**
```yaml
version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio_data:/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  qdrant_data:
  minio_data:
  redis_data:
```

---

## Week 5: é‡åŒ–æ¨¡å‹ + ModelRegistry

### Day 1-2: llama.cpp é€‚é…å™¨

**`src/adapters/inference/llamacpp_adapter.py`**
```python
"""llama.cpp é‡åŒ–æ¨¡å‹é€‚é…å™¨"""
from __future__ import annotations

import base64
import asyncio
from typing import List, Optional
from dataclasses import dataclass

from ports.inference_port import InferencePort
from domain.value_objects.verification_result import VerificationResult, VlmResponseParser
from core.evidence import EvidencePackage


@dataclass
class LlamaCppConfig:
    """llama.cpp é…ç½®"""
    model_path: str
    clip_model_path: str = ""
    n_ctx: int = 4096
    n_gpu_layers: int = -1
    n_threads: int = 4


class LlamaCppAdapter:
    """
    llama.cpp é‡åŒ–æ¨¡å‹é€‚é…å™¨
    
    ç”¨äº GGUF æ ¼å¼çš„é‡åŒ–æ¨¡å‹ï¼Œæ”¯æŒï¼š
    - CPU/GPU æ··åˆæ¨ç†
    - æä½æ˜¾å­˜å ç”¨
    - è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²
    """
    
    def __init__(self, config: LlamaCppConfig):
        from llama_cpp import Llama
        from llama_cpp.llama_chat_format import Llava15ChatHandler
        
        self.config = config
        self._parser = VlmResponseParser()
        
        # åˆå§‹åŒ–è§†è§‰å¤„ç†å™¨
        chat_handler = None
        if config.clip_model_path:
            chat_handler = Llava15ChatHandler(
                clip_model_path=config.clip_model_path
            )
        
        self.llm = Llama(
            model_path=config.model_path,
            n_ctx=config.n_ctx,
            n_gpu_layers=config.n_gpu_layers,
            n_threads=config.n_threads,
            chat_handler=chat_handler,
            verbose=False,
        )
    
    async def verify_track(
        self,
        package: EvidencePackage,
        question: str,
        plan_context: Optional[str] = None,
    ) -> VerificationResult:
        """éªŒè¯å•ä¸ªè½¨è¿¹"""
        messages = self._build_messages(package, question, plan_context)
        
        # llama.cpp æ˜¯åŒæ­¥çš„ï¼Œç”¨ run_in_executor
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: self.llm.create_chat_completion(
                messages=messages,
                max_tokens=256,
                temperature=0.1,
            )
        )
        
        raw_text = response["choices"][0]["message"]["content"]
        return self._parser.parse(raw_text)
    
    async def verify_batch(
        self,
        packages: List[EvidencePackage],
        question: str,
        plan_context: Optional[str] = None,
        concurrency: int = 1,  # llama.cpp ä¸æ”¯æŒçœŸæ­£å¹¶å‘
    ) -> List[VerificationResult]:
        """æ‰¹é‡éªŒè¯ï¼ˆä¸²è¡Œï¼‰"""
        results = []
        for pkg in packages:
            result = await self.verify_track(pkg, question, plan_context)
            results.append(result)
        return results
    
    def _build_messages(self, package, question, plan_context):
        crop_paths = package.crops[:3]  # é‡åŒ–æ¨¡å‹ç”¨æ›´å°‘å›¾ç‰‡
        
        image_contents = []
        for path in crop_paths:
            with open(path, "rb") as f:
                b64 = base64.b64encode(f.read()).decode()
                image_contents.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{b64}"}
                })
        
        return [
            {"role": "system", "content": "You are a video analysis assistant."},
            {
                "role": "user",
                "content": [
                    *image_contents,
                    {"type": "text", "text": f"Query: {question}\n\nDoes this person match? Answer with MATCH: yes or MATCH: no"}
                ]
            }
        ]
```

### Day 3-4: ModelRegistry å®ç°

**`src/adapters/inference/model_registry.py`**
```python
"""æ¨¡å‹æ³¨å†Œä¸­å¿ƒ"""
from enum import Enum
from typing import Dict, List, Optional

from ports.inference_port import InferencePort


class InferencePriority(Enum):
    """æ¨ç†ä¼˜å…ˆçº§"""
    HIGH_ACCURACY = "high_accuracy"
    LOW_LATENCY = "low_latency"
    COST_SAVING = "cost_saving"


class ModelRegistry:
    """
    æ¨¡å‹æ³¨å†Œä¸­å¿ƒ
    
    èŒè´£ï¼š
    1. ç®¡ç†å¤šä¸ªæ¨ç†é€‚é…å™¨
    2. æ ¹æ®ç­–ç•¥è·¯ç”±è¯·æ±‚
    3. æ”¯æŒè¿è¡Œæ—¶åˆ‡æ¢
    4. å®ç° A/B æµ‹è¯•
    """
    
    def __init__(self):
        self._adapters: Dict[str, InferencePort] = {}
        self._priority_map: Dict[InferencePriority, str] = {}
        self._default: Optional[str] = None
    
    def register(
        self,
        name: str,
        adapter: InferencePort,
        priorities: Optional[List[InferencePriority]] = None,
        is_default: bool = False
    ):
        """æ³¨å†Œé€‚é…å™¨"""
        self._adapters[name] = adapter
        
        if priorities:
            for priority in priorities:
                self._priority_map[priority] = name
        
        if is_default or self._default is None:
            self._default = name
    
    def get_adapter(
        self,
        priority: Optional[InferencePriority] = None
    ) -> InferencePort:
        """æ ¹æ®ä¼˜å…ˆçº§è·å–é€‚é…å™¨"""
        if priority and priority in self._priority_map:
            name = self._priority_map[priority]
            return self._adapters[name]
        
        if self._default:
            return self._adapters[self._default]
        
        raise ValueError("No adapter registered")
    
    def get_by_name(self, name: str) -> InferencePort:
        """æŒ‰åç§°è·å–"""
        if name not in self._adapters:
            raise ValueError(f"Adapter not found: {name}")
        return self._adapters[name]
    
    def list_adapters(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„é€‚é…å™¨"""
        return list(self._adapters.keys())


def create_model_registry(config) -> ModelRegistry:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæ¨¡å‹æ³¨å†Œä¸­å¿ƒ"""
    from adapters.inference.vllm_adapter import VllmAdapter, VllmConfig
    from adapters.inference.llamacpp_adapter import LlamaCppAdapter, LlamaCppConfig
    
    registry = ModelRegistry()
    
    # æ³¨å†Œ vLLM
    if getattr(config, "vllm_enabled", True):
        vllm_adapter = VllmAdapter(VllmConfig(
            endpoint=config.vllm_endpoint,
            model_name=config.vllm_model_name,
        ))
        registry.register(
            "vllm",
            vllm_adapter,
            [InferencePriority.HIGH_ACCURACY],
            is_default=True
        )
    
    # æ³¨å†Œé‡åŒ–æ¨¡å‹
    if getattr(config, "quantized_enabled", False):
        quant_adapter = LlamaCppAdapter(LlamaCppConfig(
            model_path=config.quantized_model_path,
        ))
        registry.register(
            "quantized",
            quant_adapter,
            [InferencePriority.COST_SAVING, InferencePriority.LOW_LATENCY]
        )
    
    return registry
```

### Day 5: åº”ç”¨å±‚ç”¨ä¾‹å®ç°

**`src/application/use_cases/search_tracks.py`**
```python
"""è½¨è¿¹æ£€ç´¢ç”¨ä¾‹"""
from __future__ import annotations

import time
from dataclasses import dataclass
from typing import List, Optional, TYPE_CHECKING

if TYPE_CHECKING:
    from adapters.inference.model_registry import ModelRegistry, InferencePriority
    from ports.vector_store_port import VectorStorePort


@dataclass
class SearchRequest:
    video_id: str
    question: str
    top_k: int = 5
    recall_limit: int = 50
    model_priority: Optional[str] = None


@dataclass
class SearchResult:
    track_id: int
    start_seconds: float
    end_seconds: float
    score: float
    reason: str


@dataclass
class SearchResponse:
    video_id: str
    question: str
    results: List[SearchResult]
    latency_ms: int
    model_variant: str


class SearchTracksUseCase:
    """è½¨è¿¹æ£€ç´¢ç”¨ä¾‹"""
    
    def __init__(
        self,
        model_registry: "ModelRegistry",
        vector_store: Optional["VectorStorePort"] = None,
    ):
        self.model_registry = model_registry
        self.vector_store = vector_store
    
    async def execute(self, request: SearchRequest) -> SearchResponse:
        start_time = time.time()
        
        # è·å–é€‚é…å™¨
        if request.model_priority:
            from adapters.inference.model_registry import InferencePriority
            priority = InferencePriority(request.model_priority)
            adapter = self.model_registry.get_adapter(priority)
        else:
            adapter = self.model_registry.get_adapter()
        
        # TODO: å‘é‡å¬å›ã€ç¡¬è§„åˆ™è¿‡æ»¤ã€VLM éªŒè¯
        # æš‚æ—¶è¿”å›ç©ºç»“æœ
        
        elapsed_ms = int((time.time() - start_time) * 1000)
        
        return SearchResponse(
            video_id=request.video_id,
            question=request.question,
            results=[],
            latency_ms=elapsed_ms,
            model_variant=adapter.__class__.__name__,
        )
```

---

## Week 6: å‰ç«¯éª¨æ¶

### Day 1-2: Next.js é¡¹ç›®åˆå§‹åŒ–

```bash
npx create-next-app@latest frontend --typescript --tailwind --app --src-dir
cd frontend
npm install @shadcn/ui video.js recharts zustand socket.io-client
```

### Day 3-5: æ ¸å¿ƒé¡µé¢

è¯¦è§ `final_upgrade_blueprint.md` ç¬¬ä¸ƒç« ã€‚

---

## Week 7: å®Œæ•´æµ‹è¯• + æ€§èƒ½ä¼˜åŒ–

### æµ‹è¯•è¦†ç›–

```bash
src/tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_inference_port.py
â”‚   â”œâ”€â”€ test_vllm_adapter.py
â”‚   â”œâ”€â”€ test_llamacpp_adapter.py
â”‚   â”œâ”€â”€ test_model_registry.py
â”‚   â”œâ”€â”€ test_qdrant_adapter.py
â”‚   â””â”€â”€ test_minio_adapter.py
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_search_use_case.py
â”‚   â””â”€â”€ test_api_routes.py
â””â”€â”€ e2e/
    â””â”€â”€ test_full_pipeline.py
```

---

## å¾…åˆ é™¤æ–‡ä»¶æ¸…å•

> Phase 2 ç»“æŸååˆ é™¤

| æ–‡ä»¶ | çŠ¶æ€ | ç†ç”± |
|------|------|------|
| `src/pipeline/vlm_client_hf.py` | ğŸ”´ DELETE | å·²è¢« vLLM/llama.cpp é€‚é…å™¨æ›¿ä»£ |
| `src/pipeline/recall.py` | ğŸŸ¡ REFACTOR | è¿ç§»åˆ° `application/use_cases/` |
| `src/core/config.py` | ğŸŸ¡ REFACTOR | è¿ç§»åˆ° `infrastructure/config/` |

---

## éªŒæ”¶æ ‡å‡†

- [ ] å‘é‡æ£€ç´¢å»¶è¿Ÿ < 50ms
- [ ] VLM æ¨ç†å»¶è¿Ÿ < 30s/track (vLLM)
- [ ] æ”¯æŒ vLLM / llama.cpp åˆ‡æ¢
- [ ] MinIO å­˜å‚¨å¯ç”¨
- [ ] å‰ç«¯å¯å±•ç¤ºç»“æœ
- [ ] æµ‹è¯•è¦†ç›–ç‡ > 70%

---

## ä¸‹ä¸€æ­¥

å®Œæˆ Phase 2 åï¼Œè¿›å…¥ [Phase 3: è§„æ¨¡åŒ–](./phase3_scale.md)

