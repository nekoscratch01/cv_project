# ğŸš€ Phase 1: MVP å¼€å‘è®¡åˆ’

> **ç›®æ ‡**: ä» CLI åŸå‹å‡çº§ä¸ºåˆ†å±‚æ¶æ„ + vLLM æ¨ç†æœåŠ¡  
> **å‘¨æœŸ**: 2-3 å‘¨  
> **æ ¸å¿ƒäº¤ä»˜**: `question_search()` å¯é€šè¿‡ vLLM è¿è¡Œ

---

## ç›®å½•

1. [Week 1: vLLM é›†æˆ + æ ¸å¿ƒåˆ†å±‚](#week-1-vllm-é›†æˆ--æ ¸å¿ƒåˆ†å±‚)
2. [Week 2: åŸºç¡€è®¾æ–½æ­å»º](#week-2-åŸºç¡€è®¾æ–½æ­å»º)
3. [Week 3: API å±‚ + å¼‚æ­¥ä»»åŠ¡](#week-3-api-å±‚--å¼‚æ­¥ä»»åŠ¡)
4. [å¾…åˆ é™¤æ–‡ä»¶æ¸…å•](#å¾…åˆ é™¤æ–‡ä»¶æ¸…å•)
5. [éªŒæ”¶æ ‡å‡†](#éªŒæ”¶æ ‡å‡†)

---

## Week 1: vLLM é›†æˆ + æ ¸å¿ƒåˆ†å±‚

> **ç›®æ ‡**: demo èƒ½ç”¨ vLLM è·‘èµ·æ¥

### Day 1-2: ç›®å½•ç»“æ„ + ç«¯å£å®šä¹‰

#### ä»»åŠ¡æ¸…å•

- [ ] åˆ›å»ºæ–°çš„ç›®å½•ç»“æ„
- [ ] å®šä¹‰æ ¸å¿ƒç«¯å£æ¥å£
- [ ] åˆ›å»ºé¢†åŸŸå®ä½“

#### éœ€è¦åˆ›å»ºçš„æ–‡ä»¶

```bash
# ç«¯å£å±‚
src/ports/__init__.py
src/ports/inference_port.py

# é¢†åŸŸå±‚ - å€¼å¯¹è±¡
src/domain/__init__.py
src/domain/value_objects/__init__.py
src/domain/value_objects/verification_result.py
```

#### ä»£ç å®ç°

**`src/ports/__init__.py`**
```python
"""ç«¯å£å±‚ï¼šå®šä¹‰ä¸šåŠ¡å±‚ä¾èµ–çš„æŠ½è±¡æ¥å£"""
from .inference_port import InferencePort

__all__ = ["InferencePort"]
```

**`src/ports/inference_port.py`**
```python
"""æ¨ç†ç«¯å£ï¼šä¸šåŠ¡å±‚åªä¾èµ–æ­¤æŠ½è±¡æ¥å£"""
from __future__ import annotations

from typing import Protocol, List, Optional, TYPE_CHECKING

if TYPE_CHECKING:
    from domain.value_objects.verification_result import VerificationResult
    from core.evidence import EvidencePackage


class InferencePort(Protocol):
    """
    æ¨ç†ç«¯å£åè®®
    
    è®¾è®¡åŸåˆ™ï¼š
    - ä¸šåŠ¡å±‚åªä¾èµ–æ­¤æ¥å£ï¼Œä¸ä¾èµ–å…·ä½“å®ç°
    - æ”¯æŒ vLLMã€é‡åŒ–æ¨¡å‹ã€äº‘ç«¯ API ç­‰å¤šç§åç«¯
    - å¼‚æ­¥ä¼˜å…ˆï¼Œæ”¯æŒé«˜å¹¶å‘
    """
    
    async def verify_track(
        self,
        package: "EvidencePackage",
        question: str,
        plan_context: Optional[str] = None,
    ) -> "VerificationResult":
        """
        éªŒè¯å•ä¸ªè½¨è¿¹æ˜¯å¦åŒ¹é…æŸ¥è¯¢
        
        Args:
            package: è¯æ®åŒ…ï¼ˆåŒ…å«å›¾ç‰‡è·¯å¾„ã€ç‰¹å¾ç­‰ï¼‰
            question: ç”¨æˆ·æŸ¥è¯¢
            plan_context: Router ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            VerificationResult: åŒ…å« match åˆ¤æ–­ã€ç½®ä¿¡åº¦ã€åŸå› 
        """
        ...
    
    async def verify_batch(
        self,
        packages: List["EvidencePackage"],
        question: str,
        plan_context: Optional[str] = None,
        concurrency: int = 10,
    ) -> List["VerificationResult"]:
        """
        æ‰¹é‡éªŒè¯è½¨è¿¹ï¼ˆçœŸæ­£çš„å¹¶å‘ï¼‰
        
        ä¸ HF transformers çš„ Batch ä¸åŒï¼š
        - HF Batch: åŒä¸€ä¸ª forward passï¼Œå— padding å½±å“
        - vLLM å¹¶å‘: å¤šä¸ªç‹¬ç«‹è¯·æ±‚ï¼ŒvLLM å†…éƒ¨ Continuous Batching
        """
        ...
```

**`src/domain/value_objects/verification_result.py`**
```python
"""éªŒè¯ç»“æœå€¼å¯¹è±¡"""
from __future__ import annotations

import re
from enum import Enum
from dataclasses import dataclass


class MatchStatus(Enum):
    """åŒ¹é…çŠ¶æ€æšä¸¾"""
    CONFIRMED = "confirmed"      # ç¡®è®¤åŒ¹é…
    REJECTED = "rejected"        # ç¡®è®¤ä¸åŒ¹é…
    AMBIGUOUS = "ambiguous"      # æ¨¡ç³Š/æ— æ³•åˆ¤æ–­


@dataclass(frozen=True)
class VerificationResult:
    """
    éªŒè¯ç»“æœå€¼å¯¹è±¡ï¼ˆä¸å¯å˜ï¼‰
    
    è¿™æ˜¯åè…è´¥å±‚(ACL)çš„è¾“å‡ºï¼Œå°† VLM çš„è‡ªç„¶è¯­è¨€å“åº”
    è½¬æ¢ä¸ºç³»ç»Ÿå†…éƒ¨çš„ç»“æ„åŒ–è¡¨ç¤ºã€‚
    """
    status: MatchStatus
    confidence: float
    reason: str
    raw_response: str
    
    @classmethod
    def confirmed(cls, confidence: float, reason: str, raw: str = "") -> "VerificationResult":
        return cls(MatchStatus.CONFIRMED, confidence, reason, raw)
    
    @classmethod
    def rejected(cls, reason: str, raw: str = "") -> "VerificationResult":
        return cls(MatchStatus.REJECTED, 0.0, reason, raw)
    
    @classmethod
    def error(cls, error_msg: str) -> "VerificationResult":
        return cls(MatchStatus.AMBIGUOUS, 0.0, f"Error: {error_msg}", "")
    
    @property
    def is_match(self) -> bool:
        return self.status == MatchStatus.CONFIRMED


class VlmResponseParser:
    """
    VLM å“åº”è§£æå™¨ï¼ˆåè…è´¥å±‚å®ç°ï¼‰
    
    èŒè´£ï¼š
    1. ä»è‡ªç„¶è¯­è¨€å“åº”ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
    2. å¤„ç†å„ç§è¾¹ç•Œæƒ…å†µå’Œå¼‚å¸¸æ ¼å¼
    3. å°†ä¸å¯é çš„å¤–éƒ¨æ•°æ®è½¬æ¢ä¸ºå¯é çš„å†…éƒ¨è¡¨ç¤º
    """
    
    MATCH_PATTERN = re.compile(r"MATCH:\s*(yes|no)", re.IGNORECASE)
    CONFIDENCE_PATTERN = re.compile(r"confidence[:\s]+(\d+(?:\.\d+)?)", re.IGNORECASE)
    
    def parse(self, raw_response: str) -> VerificationResult:
        """è§£æ VLM åŸå§‹å“åº”"""
        if not raw_response:
            return VerificationResult.error("Empty response")
        
        match_result = self._extract_match_marker(raw_response)
        confidence = self._extract_confidence(raw_response)
        status = self._determine_status(match_result, confidence)
        
        return VerificationResult(
            status=status,
            confidence=confidence,
            reason=self._extract_reason(raw_response),
            raw_response=raw_response
        )
    
    def _extract_match_marker(self, text: str) -> bool | None:
        match = self.MATCH_PATTERN.search(text)
        if match:
            return match.group(1).lower() == "yes"
        return None
    
    def _extract_confidence(self, text: str) -> float:
        match = self.CONFIDENCE_PATTERN.search(text)
        if match:
            try:
                conf = float(match.group(1))
                return min(max(conf, 0.0), 1.0)
            except ValueError:
                pass
        return 0.8 if self._extract_match_marker(text) is not None else 0.5
    
    def _determine_status(self, match_result: bool | None, confidence: float) -> MatchStatus:
        if match_result is True and confidence >= 0.6:
            return MatchStatus.CONFIRMED
        elif match_result is False:
            return MatchStatus.REJECTED
        else:
            return MatchStatus.AMBIGUOUS
    
    def _extract_reason(self, text: str) -> str:
        lines = text.strip().split("\n")
        reason_lines = [
            line for line in lines
            if not line.strip().lower().startswith("match:")
        ]
        return " ".join(reason_lines).strip()[:500]
```

---

### Day 3-4: vLLM é€‚é…å™¨å®ç°

#### ä»»åŠ¡æ¸…å•

- [ ] åˆ›å»º vLLM é€‚é…å™¨
- [ ] ä¿®æ”¹é…ç½®æ”¯æŒ vLLM
- [ ] ä¿®æ”¹ `_build_vlm_client()` å·¥å‚æ–¹æ³•

#### éœ€è¦åˆ›å»ºçš„æ–‡ä»¶

```bash
src/adapters/__init__.py
src/adapters/inference/__init__.py
src/adapters/inference/vllm_adapter.py
```

#### ä»£ç å®ç°

**`src/adapters/inference/vllm_adapter.py`**
```python
"""vLLM æ¨ç†é€‚é…å™¨"""
from __future__ import annotations

import base64
import asyncio
from typing import List, Optional
from dataclasses import dataclass

from openai import AsyncOpenAI

from ports.inference_port import InferencePort
from domain.value_objects.verification_result import VerificationResult, VlmResponseParser
from core.evidence import EvidencePackage


@dataclass
class VllmConfig:
    """vLLM é…ç½®"""
    endpoint: str = "http://localhost:8000/v1"
    model_name: str = "Qwen/Qwen3-VL-4B-Instruct"
    temperature: float = 0.1
    max_tokens: int = 256
    timeout: float = 120.0
    max_retries: int = 3
    max_images_per_request: int = 5


class VllmAdapter:
    """
    vLLM æ¨ç†é€‚é…å™¨
    
    å®ç° InferencePort åè®®ï¼Œé€šè¿‡ OpenAI å…¼å®¹ API è°ƒç”¨ vLLM æœåŠ¡ã€‚
    
    ç‰¹ç‚¹ï¼š
    1. å®Œå…¨æ— çŠ¶æ€ - å¯ä»¥åœ¨å¤šä¸ª worker é—´å…±äº«
    2. å¼‚æ­¥ä¼˜å…ˆ - æ”¯æŒé«˜å¹¶å‘
    3. é‡è¯•æœºåˆ¶ - ç½‘ç»œæŠ–åŠ¨å®¹é”™
    """
    
    def __init__(self, config: VllmConfig):
        self.config = config
        self.client = AsyncOpenAI(
            base_url=config.endpoint,
            api_key="EMPTY",
            timeout=config.timeout,
            max_retries=config.max_retries,
        )
        self._parser = VlmResponseParser()
    
    async def verify_track(
        self,
        package: EvidencePackage,
        question: str,
        plan_context: Optional[str] = None,
    ) -> VerificationResult:
        """éªŒè¯å•ä¸ªè½¨è¿¹"""
        try:
            messages = self._build_messages(package, question, plan_context)
            response = await self.client.chat.completions.create(
                model=self.config.model_name,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
            )
            raw_text = response.choices[0].message.content
            return self._parser.parse(raw_text)
        except Exception as e:
            return VerificationResult.error(str(e))
    
    async def verify_batch(
        self,
        packages: List[EvidencePackage],
        question: str,
        plan_context: Optional[str] = None,
        concurrency: int = 10,
    ) -> List[VerificationResult]:
        """æ‰¹é‡éªŒè¯ï¼ˆçœŸæ­£çš„å¹¶å‘è¯·æ±‚ï¼‰"""
        semaphore = asyncio.Semaphore(concurrency)
        
        async def _verify_with_limit(pkg: EvidencePackage) -> VerificationResult:
            async with semaphore:
                return await self.verify_track(pkg, question, plan_context)
        
        tasks = [_verify_with_limit(pkg) for pkg in packages]
        return await asyncio.gather(*tasks)
    
    def _build_messages(
        self,
        package: EvidencePackage,
        question: str,
        plan_context: Optional[str],
    ) -> List[dict]:
        """æ„é€  OpenAI æ ¼å¼æ¶ˆæ¯"""
        crop_paths = self._sample_crops(package.crops)
        
        image_contents = []
        for path in crop_paths:
            base64_image = self._encode_image(path)
            image_contents.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
            })
        
        prompt = self._build_prompt(package, question, plan_context)
        
        return [
            {
                "role": "system",
                "content": "You are a video analysis assistant. Answer with reasoning, then end with 'MATCH: yes' or 'MATCH: no'."
            },
            {
                "role": "user",
                "content": [*image_contents, {"type": "text", "text": prompt}]
            }
        ]
    
    def _sample_crops(self, crops: List[str]) -> List[str]:
        max_crops = self.config.max_images_per_request
        if len(crops) <= max_crops:
            return list(crops)
        step = len(crops) / max_crops
        indices = [int(i * step) for i in range(max_crops)]
        return [crops[i] for i in indices]
    
    @staticmethod
    def _encode_image(path: str) -> str:
        with open(path, "rb") as f:
            return base64.b64encode(f.read()).decode("utf-8")
    
    def _build_prompt(
        self,
        package: EvidencePackage,
        question: str,
        plan_context: Optional[str],
    ) -> str:
        motion_desc = self._build_motion_description(package)
        return f"""## Task
Verify if this person matches the query: "{question}"

## Evidence
### Appearance
The images show the person at different moments in the video.

### Motion Summary
{motion_desc}

### Constraints
{plan_context or "No additional constraints."}

## Instructions
1. Describe what you see in the images.
2. Check if the person matches the query criteria.
3. Final line must be: MATCH: yes or MATCH: no
"""
    
    def _build_motion_description(self, package: EvidencePackage) -> str:
        if not package.features:
            return "No motion data available."
        
        feats = package.features
        parts = []
        
        if feats.avg_speed_px_s < 50:
            parts.append("Standing still or barely moving")
        elif feats.avg_speed_px_s < 200:
            parts.append("Walking at normal pace")
        else:
            parts.append("Moving fast or running")
        
        dx, dy = feats.displacement_vec
        if abs(dx) > abs(dy):
            direction = "right" if dx > 0 else "left"
        else:
            direction = "down (towards camera)" if dy > 0 else "up (away)"
        parts.append(f"Moving {direction}")
        
        return ". ".join(parts) + "."
```

#### ä¿®æ”¹ç°æœ‰æ–‡ä»¶

**`src/core/config.py`** - æ·»åŠ  vLLM é…ç½®

```python
# åœ¨ SystemConfig ç±»ä¸­æ·»åŠ ï¼š

# vLLM é…ç½®
vlm_backend: str = "vllm"  # "hf" | "vllm"
vllm_endpoint: str = "http://localhost:8000/v1"
vllm_model_name: str = "Qwen/Qwen3-VL-4B-Instruct"
```

**`src/pipeline/video_semantic_search.py`** - ä¿®æ”¹å·¥å‚æ–¹æ³•

```python
def _build_vlm_client(self):
    if self.config.vlm_backend == "vllm":
        from adapters.inference.vllm_adapter import VllmAdapter, VllmConfig
        return VllmAdapter(VllmConfig(
            endpoint=self.config.vllm_endpoint,
            model_name=self.config.vllm_model_name,
            temperature=self.config.vlm_temperature,
            max_tokens=self.config.vlm_max_new_tokens,
        ))
    elif self.config.vlm_backend in {"hf", "transformers"}:
        from pipeline.vlm_client_hf import Qwen3VL4BHFClient
        return Qwen3VL4BHFClient(self.config)
    else:
        raise RuntimeError(f"Unknown vlm_backend: {self.config.vlm_backend}")
```

---

### Day 5: vLLM æœåŠ¡éƒ¨ç½² + ç«¯åˆ°ç«¯æµ‹è¯•

#### ä»»åŠ¡æ¸…å•

- [ ] åœ¨ Colab ä¸Šéƒ¨ç½² vLLM æœåŠ¡
- [ ] ç¼–å†™ vLLM å¯åŠ¨è„šæœ¬
- [ ] ç«¯åˆ°ç«¯æµ‹è¯• `question_search()`
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•

#### vLLM å¯åŠ¨è„šæœ¬

**`deploy/start_vllm.sh`**
```bash
#!/bin/bash
# vLLM æœåŠ¡å¯åŠ¨è„šæœ¬

# åŸºç¡€ç‰ˆæœ¬ï¼ˆFP16ï¼‰
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-VL-4B-Instruct \
    --trust-remote-code \
    --host 0.0.0.0 \
    --port 8000 \
    --gpu-memory-utilization 0.90 \
    --max-model-len 8192 \
    --limit-mm-per-prompt image=5 \
    --enable-prefix-caching

# AWQ é‡åŒ–ç‰ˆæœ¬ï¼ˆæ›´å¿«ï¼Œæ˜¾å­˜æ›´å°ï¼‰
# python -m vllm.entrypoints.openai.api_server \
#     --model Qwen/Qwen3-VL-4B-Instruct-AWQ \
#     --trust-remote-code \
#     --quantization awq \
#     --host 0.0.0.0 \
#     --port 8000 \
#     --gpu-memory-utilization 0.90 \
#     --max-model-len 8192 \
#     --limit-mm-per-prompt image=5
```

#### Colab Notebook ä»£ç 

```python
# Cell 1: å®‰è£…ä¾èµ–
!pip install vllm openai

# Cell 2: å¯åŠ¨ vLLM æœåŠ¡ï¼ˆåå°è¿è¡Œï¼‰
import subprocess
import time

process = subprocess.Popen([
    "python", "-m", "vllm.entrypoints.openai.api_server",
    "--model", "Qwen/Qwen3-VL-4B-Instruct",
    "--trust-remote-code",
    "--host", "0.0.0.0",
    "--port", "8000",
    "--gpu-memory-utilization", "0.90",
    "--max-model-len", "8192",
    "--limit-mm-per-prompt", "image=5",
])

print("Waiting for vLLM to start...")
time.sleep(120)  # ç­‰å¾…æ¨¡å‹åŠ è½½

# Cell 3: æµ‹è¯• vLLM æœåŠ¡
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")
response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-4B-Instruct",
    messages=[{"role": "user", "content": "Hello!"}],
    max_tokens=50
)
print(response.choices[0].message.content)

# Cell 4: è¿è¡Œ demo
%cd /content/cv_project
!python -c "
from pipeline.video_semantic_search import VideoSemanticSystem

system = VideoSemanticSystem()
system.build_index()
system.question_search('Find the person in blue')
"
```

#### å•å…ƒæµ‹è¯•

**`tests/unit/test_vllm_adapter.py`**
```python
import pytest
from unittest.mock import AsyncMock, MagicMock, patch

from adapters.inference.vllm_adapter import VllmAdapter, VllmConfig
from domain.value_objects.verification_result import MatchStatus


@pytest.fixture
def mock_config():
    return VllmConfig(
        endpoint="http://localhost:8000/v1",
        model_name="test-model"
    )


@pytest.fixture
def adapter(mock_config):
    with patch("adapters.inference.vllm_adapter.AsyncOpenAI"):
        return VllmAdapter(mock_config)


class TestVlmResponseParser:
    def test_parse_match_yes(self):
        from domain.value_objects.verification_result import VlmResponseParser
        
        parser = VlmResponseParser()
        result = parser.parse("The person is wearing blue. MATCH: yes")
        
        assert result.status == MatchStatus.CONFIRMED
        assert result.is_match is True
    
    def test_parse_match_no(self):
        from domain.value_objects.verification_result import VlmResponseParser
        
        parser = VlmResponseParser()
        result = parser.parse("No match found. MATCH: no")
        
        assert result.status == MatchStatus.REJECTED
        assert result.is_match is False
    
    def test_parse_empty_response(self):
        from domain.value_objects.verification_result import VlmResponseParser
        
        parser = VlmResponseParser()
        result = parser.parse("")
        
        assert result.status == MatchStatus.AMBIGUOUS


@pytest.mark.asyncio
async def test_verify_track(adapter):
    # Mock the OpenAI response
    mock_response = MagicMock()
    mock_response.choices = [MagicMock(message=MagicMock(content="MATCH: yes"))]
    adapter.client.chat.completions.create = AsyncMock(return_value=mock_response)
    
    # Mock package
    mock_package = MagicMock()
    mock_package.crops = ["path/to/image.jpg"]
    mock_package.features = None
    
    with patch.object(adapter, "_encode_image", return_value="base64data"):
        result = await adapter.verify_track(mock_package, "test question")
    
    assert result.is_match is True
```

---

## Week 2: åŸºç¡€è®¾æ–½æ­å»º

> **ç›®æ ‡**: æ­å»ºæ•°æ®åº“ã€æ—¥å¿—ã€é…ç½®ç®¡ç†

### Day 1-2: é…ç½®ç³»ç»Ÿé‡æ„

#### éœ€è¦åˆ›å»ºçš„æ–‡ä»¶

```bash
src/infrastructure/__init__.py
src/infrastructure/config/__init__.py
src/infrastructure/config/app_config.py
src/infrastructure/config/infra_config.py
```

#### ä»£ç å®ç°

**`src/infrastructure/config/app_config.py`**
```python
"""åº”ç”¨é…ç½®ï¼ˆä¸šåŠ¡å±‚ï¼‰"""
from pydantic_settings import BaseSettings
from typing import Optional


class AppConfig(BaseSettings):
    """åº”ç”¨é…ç½®"""
    
    # æœåŠ¡
    service_name: str = "edge-detective"
    debug: bool = False
    
    # æ¨ç†
    vlm_backend: str = "vllm"
    vllm_endpoint: str = "http://localhost:8000/v1"
    vllm_model_name: str = "Qwen/Qwen3-VL-4B-Instruct"
    
    # ä¸šåŠ¡å‚æ•°
    default_top_k: int = 5
    default_recall_limit: int = 50
    max_images_per_request: int = 5
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
```

### Day 3-4: ç»“æ„åŒ–æ—¥å¿—

**`src/infrastructure/logging/structured_logger.py`**
```python
"""ç»“æ„åŒ–æ—¥å¿—é…ç½®"""
import structlog
import logging


def configure_logging(debug: bool = False):
    """é…ç½®ç»“æ„åŒ–æ—¥å¿—"""
    log_level = logging.DEBUG if debug else logging.INFO
    
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        wrapper_class=structlog.stdlib.BoundLogger,
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )
    
    logging.basicConfig(level=log_level)


def get_logger(name: str = __name__):
    return structlog.get_logger(name)
```

### Day 5: Mock å­˜å‚¨é€‚é…å™¨

**`src/adapters/storage/memory_repo.py`**
```python
"""å†…å­˜å­˜å‚¨ï¼ˆå¼€å‘/æµ‹è¯•ç”¨ï¼‰"""
from typing import Dict, List, Optional
from dataclasses import dataclass, field


@dataclass
class InMemoryVideoRepository:
    """è§†é¢‘ä»“å‚¨å†…å­˜å®ç°"""
    _videos: Dict[str, dict] = field(default_factory=dict)
    
    async def save(self, video_id: str, data: dict):
        self._videos[video_id] = data
    
    async def get(self, video_id: str) -> Optional[dict]:
        return self._videos.get(video_id)
    
    async def list_all(self) -> List[dict]:
        return list(self._videos.values())


@dataclass
class InMemoryTrackRepository:
    """è½¨è¿¹ä»“å‚¨å†…å­˜å®ç°"""
    _tracks: Dict[str, dict] = field(default_factory=dict)
    
    async def save(self, video_id: str, track_id: int, data: dict):
        key = f"{video_id}_{track_id}"
        self._tracks[key] = data
    
    async def get_by_video(self, video_id: str) -> List[dict]:
        return [
            t for k, t in self._tracks.items()
            if k.startswith(f"{video_id}_")
        ]
```

---

## Week 3: API å±‚ + å¼‚æ­¥ä»»åŠ¡

> **ç›®æ ‡**: FastAPI æœåŠ¡ + Celery ä»»åŠ¡é˜Ÿåˆ—

### Day 1-2: FastAPI éª¨æ¶

**`src/api/main.py`**
```python
"""FastAPI å…¥å£"""
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from api.routes import health, search

app = FastAPI(
    title="Edge-Detective API",
    version="2.0.0",
    description="è§†é¢‘è¯­ä¹‰æ£€ç´¢æœåŠ¡"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(health.router, tags=["health"])
app.include_router(search.router, prefix="/api/v1", tags=["search"])


@app.on_event("startup")
async def startup():
    from infrastructure.logging.structured_logger import configure_logging
    configure_logging()


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

**`src/api/routes/health.py`**
```python
from fastapi import APIRouter

router = APIRouter()


@router.get("/health")
async def health_check():
    return {"status": "healthy"}


@router.get("/health/ready")
async def readiness():
    return {"ready": True}


@router.get("/health/live")
async def liveness():
    return {"alive": True}
```

**`src/api/routes/search.py`**
```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Optional

router = APIRouter()


class SearchRequest(BaseModel):
    video_id: str
    question: str
    top_k: int = 5


class TrackResult(BaseModel):
    track_id: int
    start_s: float
    end_s: float
    score: float
    reason: str


class SearchResponse(BaseModel):
    video_id: str
    question: str
    results: List[TrackResult]
    latency_ms: int


@router.post("/search", response_model=SearchResponse)
async def search_tracks(request: SearchRequest):
    # TODO: æ¥å…¥ SearchTracksUseCase
    raise HTTPException(status_code=501, detail="Not implemented yet")
```

### Day 3-4: Celery ä»»åŠ¡é˜Ÿåˆ—

**`src/tasks/celery_app.py`**
```python
from celery import Celery

app = Celery(
    "edge_detective",
    broker="redis://localhost:6379/0",
    backend="redis://localhost:6379/0"
)

app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
)
```

**`src/tasks/indexing.py`**
```python
from tasks.celery_app import app


@app.task(bind=True)
def index_video_task(self, video_path: str, video_id: str):
    """åå°ç´¢å¼•ä»»åŠ¡"""
    self.update_state(state="PROCESSING", meta={"progress": 0})
    
    # TODO: è°ƒç”¨ IndexVideoUseCase
    
    return {"status": "completed", "video_id": video_id}
```

### Day 5: æ•´åˆæµ‹è¯•

```bash
# å¯åŠ¨ Redis
docker run -d -p 6379:6379 redis:7-alpine

# å¯åŠ¨ Celery Worker
celery -A tasks.celery_app worker --loglevel=info

# å¯åŠ¨ FastAPI
uvicorn api.main:app --reload --port 8080

# æµ‹è¯•
curl http://localhost:8080/health
```

---

## å¾…åˆ é™¤æ–‡ä»¶æ¸…å•

> Phase 1 ç»“æŸåï¼Œä»¥ä¸‹æ–‡ä»¶æ ‡è®°ä¸º **DEPRECATED**ï¼ŒPhase 2 å¼€å§‹æ—¶åˆ é™¤

| æ–‡ä»¶ | çŠ¶æ€ | ç†ç”± | æ›¿ä»£ |
|------|------|------|------|
| `src/pipeline/vlm_client_hf.py` | ğŸ”´ DEPRECATED | ç›´æ¥åœ¨ä¸šåŠ¡è¿›ç¨‹åŠ è½½æ¨¡å‹ | `adapters/inference/vllm_adapter.py` |
| `src/api/` (æ—§ç‰ˆ) | ğŸ”´ DELETE | å¦‚æœå­˜åœ¨æ—§ API ä»£ç  | `src/api/` (æ–°ç‰ˆ) |
| `src/tasks/` (æ—§ç‰ˆ) | ğŸ”´ DELETE | å¦‚æœå­˜åœ¨æ—§ä»»åŠ¡ä»£ç  | `src/tasks/` (æ–°ç‰ˆ) |

---

## éªŒæ”¶æ ‡å‡†

### Week 1 éªŒæ”¶

- [ ] `question_search()` å¯é€šè¿‡ vLLM è¿è¡Œ
- [ ] `ports/inference_port.py` å®šä¹‰å®Œæˆ
- [ ] `adapters/inference/vllm_adapter.py` å®ç°å®Œæˆ
- [ ] å•å…ƒæµ‹è¯•é€šè¿‡

### Week 2 éªŒæ”¶

- [ ] é…ç½®ç³»ç»Ÿé‡æ„å®Œæˆ
- [ ] ç»“æ„åŒ–æ—¥å¿—å¯ç”¨
- [ ] Mock å­˜å‚¨é€‚é…å™¨å¯ç”¨

### Week 3 éªŒæ”¶

- [ ] FastAPI æœåŠ¡å¯å¯åŠ¨
- [ ] `/health` ç«¯ç‚¹å¯è®¿é—®
- [ ] Celery ä»»åŠ¡å¯æ‰§è¡Œ
- [ ] Docker ç¯å¢ƒå¯è¿è¡Œ

---

## ä¸‹ä¸€æ­¥

å®Œæˆ Phase 1 åï¼Œè¿›å…¥ [Phase 2: ç”Ÿäº§å¯ç”¨](./phase2_production.md)

